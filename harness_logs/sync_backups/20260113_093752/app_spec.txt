# Noodle 2 — Product Description

## Executive Summary

**Noodle 2** is a safety-aware, policy-driven orchestration system for large-scale physical-design (PD) experimentation built on top of OpenROAD. It manages *uncertainty, failure, and limited compute budgets* while exploring Engineering Change Orders (ECOs) across complex, multi-stage design workflows.

Noodle 2 does **not** replace PD tools or algorithms. Instead, it provides a deterministic control plane that enables structured experimentation, auditable decision-making, and safe automation across multiple studies, stages, and design variants.

---

## Positioning

Noodle 2 occupies the layer **above** physical-design engines such as OpenROAD:

* OpenROAD solves placement, routing, and timing problems.
* Noodle 2 decides **what to try, when to stop, and what to trust**.

It acts as a *safety-critical experiment controller* rather than an optimizer or solver.

---

## Core Concepts

### Study

A **Study** is the top-level unit of work in Noodle 2.

A Study defines:

* a base design snapshot (e.g., Nangate45, ASAP7, Sky130)
* a safety domain
* policy and rail configuration
* one or more multi-stage experiment graphs

Studies are isolated by default: telemetry, priors, and outcomes do not leak across studies unless explicitly allowed via warm-start configuration.

**Study Definition Schema**:

```yaml
# study_definition.yaml
study:
  name: nangate45_extreme_fix
  version: "1.0"
  pdk: nangate45
  safety_domain: guarded

  snapshot:
    path: ./snapshots/nangate45_extreme
    hash: sha256:abc123def456...

  objective:
    mode: pareto                    # "pareto", "weighted_sum", or "lexicographic"
    primary: wns_ps
    secondary: hot_ratio
    weights: [0.7, 0.3]

  diagnosis:
    enabled: true
    timing_paths: 20
    hotspot_threshold: 0.7

  visualization:
    enabled: true
    heatmaps: [placement_density, routing_congestion, rudy]
    differential: true
    critical_path_overlay: true
    pareto_frontier: true

  stages:
    - name: exploration
      mode: sta_only
      budget: 50
      survivors: 10
      eco_classes: [topology_neutral, placement_local]

    - name: refinement
      mode: sta_congestion
      budget: 30
      survivors: 3
      eco_classes: [placement_local, routing_affecting]

    - name: closure
      mode: full_verification
      budget: 10
      survivors: 1
      eco_classes: [topology_neutral, placement_local]

  rails:
    abort:
      wns_ps: -10000
      timeout_seconds: 600
    stage:
      failure_rate: 0.8
    study:
      catastrophic_failures: 5
      max_runtime_hours: 4

  checkpointing:
    enabled: true
    interval: per_stage

  priors:
    mode: warm_start               # "fresh" or "warm_start"
    source_studies: []             # list of prior study names
```

---

### Case

A **Case** represents a concrete design state derived from a Study.

* The **base Case** is the original snapshot.
* **Derived Cases** are created by applying one or more ECOs.
* Cases form a directed acyclic graph (DAG) within a Study.

**Deterministic naming contract**:

```
<case_name>_<stage_index>_<derived_index>
```

Examples:
* `nangate45_0_0` — base case, stage 0
* `nangate45_1_3` — stage 1, derived case 3

This guarantees traceable lineage across complex branching experiments.

---

### Stage

A **Stage** is a refinement phase within a Study.

* Studies may define any number of ordered stages (`N >= 1`).
* Each stage specifies:

  * **execution mode** (e.g., STA-only, STA+congestion)
  * **trial budget**: maximum number of ECO trials to attempt
  * **survivor count**: how many top-performing Cases advance to the next stage
  * **allowed ECO classes**: which risk categories of ECOs may be applied
  * **abort thresholds**: metric limits that trigger early termination
  * **safety rails**: hard limits that cannot be exceeded
  * **approval gate** (optional): require human approval before proceeding

**Survivor Selection**:

At stage completion, Cases are ranked and selected using configurable methods:

| Method | Description |
|--------|-------------|
| `pure_top_n` | Select N best by objective function |
| `diverse_top_n` | Select N best while maintaining diversity |
| `tournament` | Tournament selection with randomness |
| `pareto_front` | Select cases on the Pareto frontier |

**Diversity-Aware Selection**:

```yaml
survivor_selection:
  method: diverse_top_n
  count: 5
  diversity:
    enabled: true
    metric: eco_path_distance    # how different the ECO sequences are
    min_diversity: 0.3           # minimum distance between survivors
  elitism:
    always_keep_best: true
    random_survivors: 1          # keep 1 random survivor for exploration
```

Stages execute sequentially; only surviving Cases advance.

---

### ECO (Engineering Change Order)

An **ECO** is a first-class, auditable unit of change.

Each ECO:

* has a stable name and classification
* emits metrics, logs, and failure semantics
* is comparable across Cases and Studies
* belongs to an explicit ECO class with a defined risk envelope

**ECO Definition Schema**:

```yaml
eco:
  name: resize_critical_drivers
  eco_class: placement_local
  description: Upsize cells on critical timing paths

  # When to consider this ECO (auto-diagnosis integration)
  preconditions:
    requires_timing_issue: true
    wns_ps_worse_than: -500
    hot_ratio_below: 0.5          # don't apply if congestion is the main problem

  # TCL template with parameters
  tcl_template: |
    # Find critical paths
    set paths [find_timing_paths -max_paths {{ max_paths }}]

    foreach path $paths {
      set driver [get_driver $path]
      set current_size [get_cell_size $driver]
      set new_size [expr {$current_size * {{ size_multiplier }}}]

      # Resize if within bounds
      if {$new_size <= {{ max_size }}} {
        resize_cell $driver -size $new_size
      }
    }

  # Configurable parameters
  parameters:
    max_paths:
      type: int
      default: 10
      range: [5, 50]
    size_multiplier:
      type: float
      default: 1.5
      range: [1.1, 2.0]
    max_size:
      type: float
      default: 4.0

  # Expected effects (for auto-diagnosis suggestions)
  expected_effects:
    wns_ps: improve
    tns_ps: improve
    area_um2: increase_slightly
    power_mw: increase_slightly

  # Post-execution verification
  postconditions:
    wns_must_improve: true
    no_new_drc_violations: true

  timeout_seconds: 300
```

**Compound ECOs**:

Multiple ECOs can be composed into a single unit:

```yaml
compound_eco:
  name: timing_rescue_combo
  description: Combined aggressive timing fix

  components:
    - eco: resize_critical_drivers
      parameters:
        size_multiplier: 1.3
        max_paths: 20
    - eco: insert_buffers
      parameters:
        max_buffers: 50
    - eco: swap_to_faster_cells
      parameters:
        path_count: 10

  apply_order: sequential
  rollback_on_failure: all        # "all", "partial", or "none"

  eco_class: placement_local      # inherited from most restrictive component
```

ECOs do not modify OpenROAD internals and are executed through standardized helper APIs.

---

### Rails (Policy Limits)

**Rails** are hard policy limits that constrain Noodle 2's behavior. Unlike soft thresholds that trigger warnings, rails trigger immediate action.

Rail types:

* **abort_rail**: if crossed, stop the current trial immediately
* **stage_rail**: if crossed, terminate the entire stage
* **study_rail**: if crossed, halt the entire Study

Example rail configuration:

```yaml
rails:
  abort:
    wns_ps: -5000             # abort trial if WNS worse than -5ns
    timeout_seconds: 600      # abort trial if exceeds 10 minutes
  stage:
    failure_rate: 0.8         # stop stage if 80% of trials fail
  study:
    catastrophic_failures: 3  # stop study after 3 catastrophic failures
    max_runtime_hours: 8      # stop study after 8 hours total
```

---

### Priors (ECO Effectiveness History)

**Priors** track the historical effectiveness of ECOs within a Study.

Prior states:

* **unknown**: ECO has not been executed yet (default initial state)
* **trusted**: ECO has consistently improved metrics without failures
* **mixed**: ECO has shown variable results (some improvements, some regressions)
* **suspicious**: ECO has caused failures or significant regressions

Priors influence ECO selection and trial ordering. Suspicious ECOs may be deprioritized or excluded based on safety domain.

**Warm Start** (Cross-Study Prior Transfer):

```yaml
priors:
  mode: warm_start
  source_studies:
    - name: nangate45_fix_v1
      weight: 0.8
    - name: nangate45_fix_v2
      weight: 0.5
  decay: 0.9                      # reduce confidence for older priors
  conflict_resolution: newest     # "newest", "highest_weight", or "average"
```

---

### Promotion Rules

**Promotion rules** govern when and how Cases advance through stages:

* **metric_threshold**: Case must meet minimum improvement targets
* **relative_ranking**: Case must be in top N by objective function
* **pareto_membership**: Case must be on the Pareto frontier
* **safety_clearance**: Case must not have triggered any rails
* **stability_check**: Case must produce consistent results across repeated trials (optional)

---

## Safety Model

### Safety Domains

Every Study runs under a declared **Safety Domain**:

| Domain | Description | Use Case |
|--------|-------------|----------|
| `sandbox` | Exploratory, permissive; all ECO classes allowed | Early experimentation |
| `guarded` | Default, production-like; moderate restrictions | Standard development |
| `locked` | Conservative, regression-only; minimal changes allowed | Pre-tapeout, CI gates |

Safety Domains constrain:

* allowed ECO classes
* abort sensitivity (how quickly to stop on anomalies)
* promotion rules (how strict the advancement criteria)
* use of historical priors (whether to trust past results)

---

### ECO Classes

ECOs are categorized by **blast radius** (scope of potential impact):

| Class | Description | Typical Operations | Risk Level |
|-------|-------------|-------------------|------------|
| `topology_neutral` | No netlist changes | Timing analysis, reporting | Lowest |
| `placement_local` | Cell movement within small region | Buffer insertion, gate sizing | Low |
| `routing_affecting` | May require route updates | Net restructuring, pin swaps | Medium |
| `global_disruptive` | Major structural changes | Floorplan modification, macro moves | Highest |

**Safety Domain ECO Restrictions**:

| Domain | Allowed ECO Classes |
|--------|---------------------|
| `sandbox` | All classes |
| `guarded` | `topology_neutral`, `placement_local`, `routing_affecting` |
| `locked` | `topology_neutral`, `placement_local` only |

---

### Legality & Guardrails

Before execution, Noodle 2 produces a **Run Legality Report** summarizing:

* declared safety domain
* proposed ECO classes
* applicable rails
* abort criteria

**Illegal runs are rejected before consuming compute.**

Example legality check failure:

```
LEGALITY CHECK FAILED
  Safety domain: locked
  Requested ECO class: global_disruptive
  Reason: global_disruptive ECOs not permitted in locked domain
  Action: Run rejected
```

---

## Auto-Diagnosis

Noodle 2 includes an **auto-diagnosis** system that analyzes design metrics to identify root causes and suggest appropriate ECOs.

### Timing Diagnosis

```yaml
diagnosis:
  timing:
    enabled: true
    analyze_critical_paths: true
    path_count: 20
    classify_bottleneck: true     # wire-dominated vs cell-dominated
    identify_problem_cells: true
    slack_histogram: true
```

**Timing Diagnosis Output**:

```json
{
  "timing_diagnosis": {
    "wns_ps": -2150,
    "tns_ps": -45000,
    "failing_endpoints": 234,

    "dominant_issue": "wire_dominated",
    "confidence": 0.85,

    "critical_region": {
      "bbox": {"x1": 100, "y1": 150, "x2": 200, "y2": 250},
      "description": "northeast quadrant near macro M1"
    },

    "problem_nets": [
      {"name": "data_bus[31]", "slack_ps": -2150, "wire_delay_pct": 78},
      {"name": "data_bus[30]", "slack_ps": -1980, "wire_delay_pct": 72}
    ],

    "suggested_ecos": [
      {"eco": "insert_buffers", "priority": 1, "reason": "wire-dominated paths"},
      {"eco": "spread_dense_region", "priority": 2, "reason": "reduce detours"}
    ],

    "slack_histogram": {
      "bins_ps": [-3000, -2000, -1000, 0],
      "counts": [12, 45, 177, 1024]
    }
  }
}
```

### Congestion Diagnosis

```yaml
diagnosis:
  congestion:
    enabled: true
    identify_hotspots: true
    hotspot_threshold: 0.7
    correlate_with_placement: true
    analyze_layer_distribution: true
```

**Congestion Diagnosis Output**:

```json
{
  "congestion_diagnosis": {
    "hot_ratio": 0.35,
    "overflow_total": 1247,

    "hotspot_count": 3,
    "hotspots": [
      {
        "id": 1,
        "bbox": {"x1": 50, "y1": 200, "x2": 100, "y2": 250},
        "severity": "critical",
        "cause": "pin_crowding",
        "affected_layers": ["metal3", "metal4"],
        "suggested_ecos": ["reroute_congested_nets", "spread_dense_region"]
      },
      {
        "id": 2,
        "bbox": {"x1": 150, "y1": 100, "x2": 200, "y2": 150},
        "severity": "moderate",
        "cause": "placement_density",
        "affected_layers": ["metal2", "metal3"],
        "suggested_ecos": ["spread_dense_region"]
      }
    ],

    "layer_breakdown": {
      "metal2": {"usage_pct": 85, "overflow": 234},
      "metal3": {"usage_pct": 92, "overflow": 567},
      "metal4": {"usage_pct": 78, "overflow": 123}
    },

    "correlation_with_placement": {
      "placement_density_correlation": 0.72,
      "macro_proximity_correlation": 0.45
    }
  }
}
```

### Combined Diagnosis Report

The auto-diagnosis system produces a combined report that drives ECO selection:

```json
{
  "diagnosis_summary": {
    "primary_issue": "timing",
    "secondary_issue": "congestion",

    "recommended_strategy": "timing_first",
    "reasoning": "WNS is severe (-2150ps), congestion is moderate (0.35). Fix timing first, then address residual congestion.",

    "eco_priority_queue": [
      {"eco": "insert_buffers", "priority": 1, "addresses": "timing"},
      {"eco": "resize_critical_drivers", "priority": 2, "addresses": "timing"},
      {"eco": "spread_dense_region", "priority": 3, "addresses": "both"},
      {"eco": "reroute_congested_nets", "priority": 4, "addresses": "congestion"}
    ]
  }
}
```

---

## Experiment Control

### Multi-Stage Execution

Noodle 2 supports **arbitrary N-stage experiment graphs**:

* coarse exploration -> focused refinement -> conservative closure
* per-stage policy binding
* per-stage safety evaluation
* optional human approval gates

Example 4-stage flow with approval gate:

```yaml
stages:
  - name: exploration
    mode: sta_only
    budget: 50
    survivors: 10
    eco_classes: [topology_neutral, placement_local]
    visualization:
      heatmaps: true
      differential: false         # no baseline yet

  - name: refinement
    mode: sta_congestion
    budget: 30
    survivors: 3
    eco_classes: [placement_local, routing_affecting]
    visualization:
      heatmaps: true
      differential: true
      pareto_frontier: true

  - name: approval_gate
    type: human_approval
    timeout_hours: 24
    show_summary: true
    show_visualizations: true
    required_approvers: 1
    message: "Review results before aggressive ECOs"

  - name: aggressive_closure
    mode: full_verification
    budget: 15
    survivors: 1
    eco_classes: [placement_local, routing_affecting]
    requires_approval: approval_gate
    visualization:
      heatmaps: true
      differential: true
      critical_path_overlay: true
```

This generalizes traditional Stage 1 / Stage 2 flows without loss of determinism.

---

### Failure Detection & Containment

Each trial is deterministically classified immediately after execution.

**Early Failure Classification**:

| Type | Severity | Description |
|------|----------|-------------|
| `tool_crash` | critical | OpenROAD process exited non-zero |
| `timeout` | high | Trial exceeded time limit |
| `parse_failure` | high | Required output files missing or malformed |
| `metric_regression` | medium | Metrics significantly worse than baseline |
| `rail_violation` | medium | Trial crossed an abort rail |
| `postcondition_failure` | medium | ECO postconditions not satisfied |
| `visualization_unavailable` | low | Heatmap export failed (non-blocking) |

**Containment Scopes**:

Failures are contained at the appropriate scope:

* **individual ECO**: mark ECO prior as suspicious, continue stage
* **ECO class**: disable entire class for remainder of stage
* **stage**: abort stage, advance only current survivors
* **entire Study**: halt all execution, require human review

---

### Adaptive Policy with Memory

Within a Study, Noodle 2 maintains structured memory:

* ECO effectiveness history (priors)
* early-failure statistics per ECO and ECO class
* catastrophic failure markers
* diagnosis history (what issues were detected and fixed)

Policies adapt conservatively based on evidence while remaining inspectable and deterministic.

**Adaptation Rules**:

```
IF eco.consecutive_failures >= 3:
    eco.prior = "suspicious"

IF eco_class.failure_rate > 0.5:
    eco_class.deprioritize()

IF stage.catastrophic_failures >= study.rail.catastrophic_failures:
    study.halt("rail_violation")

IF eco.postcondition_failures >= 2:
    eco.prior = "mixed"
    eco.requires_diagnosis_match = true  # only apply when diagnosis suggests it
```

---

### Checkpoint & Resume

Noodle 2 supports checkpointing for long-running studies:

```yaml
checkpointing:
  enabled: true
  interval: per_stage            # "per_stage", "per_trial", or "hourly"
  location: ./checkpoints/
  retention: 3                   # keep last 3 checkpoints
  include_artifacts: false       # artifacts stored separately
```

**Checkpoint Contents**:

```
checkpoints/
  nangate45_fix/
    stage_0_complete.noodle      # checkpoint file
    stage_1_complete.noodle
    latest -> stage_1_complete.noodle
```

**CLI**:

```bash
# Resume from latest checkpoint
noodle2 resume --study nangate45_fix

# Resume from specific checkpoint
noodle2 resume --checkpoint ./checkpoints/nangate45_fix/stage_0_complete.noodle

# List available checkpoints
noodle2 checkpoints --study nangate45_fix
```

---

### Replay & Debug Mode

Noodle 2 supports replaying specific trials for debugging:

```bash
# Replay specific trial with verbose output
noodle2 replay --study nangate45_fix --case nangate45_1_5 --verbose

# Replay with modified ECO parameters
noodle2 replay --study nangate45_fix --case nangate45_1_3 \
  --eco resize_critical_drivers \
  --param size_multiplier=1.8

# Replay with full visualization (even if originally disabled)
noodle2 replay --case nangate45_1_5 --force-visualization

# Generate detailed debug report
noodle2 debug --case nangate45_1_5 --output debug_report/
```

**Debug Report Contents**:

```
debug_report/
  case_nangate45_1_5/
    execution_trace.json         # step-by-step execution log
    tcl_commands.log             # exact TCL sent to OpenROAD
    openroad_stdout.log          # full OpenROAD output
    openroad_stderr.log
    metrics_before.json
    metrics_after.json
    diagnosis_at_execution.json
    eco_parameters_used.json
    all_heatmaps/                # full visualization set
```

---

## Telemetry & Auditability

Noodle 2 emits structured telemetry across three axes:

* **Study-level**: overall progress, final outcomes, aggregate statistics
* **Stage-level**: survivor selection, failure rates, timing improvements
* **Case-level**: individual trial results, ECO effectiveness, metric deltas

**Artifact Bundle Structure**:

```
artifacts/
  <study_name>/
    study_summary.json
    study_config.yaml              # frozen study definition
    diagnosis_history.json         # all diagnosis reports
    pareto_history.json            # Pareto frontier evolution

    <case_name>/
      <stage_index>/
        <trial_index>/
          artifact_index.json      # manifest of all artifacts
          metrics.json             # parsed timing/congestion metrics
          diagnosis.json           # auto-diagnosis for this state
          timing_report.txt        # raw report_checks output
          congestion_report.txt    # raw global_route output
          eco_applied.json         # ECO details and parameters

          visualizations/          # all visualization artifacts
            heatmaps/
              placement_density.csv
              placement_density.png
              routing_congestion.csv
              routing_congestion.png
              rudy.csv
              rudy.png
            differential/          # comparison with parent case
              placement_density_diff.png
              routing_congestion_diff.png
            overlays/
              critical_paths.png
              hotspots.png

          logs/
            openroad.log
            eco_execution.log
          safety_trace.json        # rail checks, failure classifications

    comparisons/                   # cross-case comparisons
      stage_0_survivors.json
      stage_1_survivors.json
      final_comparison.json

    visualizations/                # study-level visualizations
      pareto_frontier.png
      pareto_evolution.gif
      improvement_trajectory.png
      stage_progression.png
```

All schemas are additive and backward-compatible.

---

## Visualization System

Noodle 2 treats visualization as a **first-class, high-priority feature**. Visual evidence is essential for understanding *where* and *why* ECOs succeed or fail.

### Visualization Modes

| Mode | Description | When Used |
|------|-------------|-----------|
| **Headless (MVP)** | Xvfb-based, fully automated | Ray workers, CI, production |
| **Interactive (Follow-up)** | X11 passthrough, manual exploration | Developer workstations |

### Heatmap Types

| Heatmap | Description | Primary Use |
|---------|-------------|-------------|
| `placement_density` | Cell distribution across die | Identify placement hotspots |
| `routing_congestion` | Post-GR wire density | Identify routing bottlenecks |
| `rudy` | Rectangular Uniform wire Density | Pre-GR congestion estimate |
| `power_density` | Power consumption distribution | IR drop correlation |
| `timing_slack` | Slack distribution (custom) | Critical region identification |

### Headless Execution (MVP)

```bash
# Inside container with Xvfb
Xvfb :99 -screen 0 1024x768x24 &
export DISPLAY=:99

openroad -gui -exit <<EOF
source load_design.tcl

# Placement density
gui::select_heatmap "Placement Density"
gui::dump_heatmap placement_density heatmaps/placement_density.csv

# Routing congestion (requires global_route first)
global_route -congestion_report_file congestion_report.txt
gui::select_heatmap "Routing Congestion"
gui::dump_heatmap routing_congestion heatmaps/routing_congestion.csv

# RUDY
gui::select_heatmap "RUDY"
gui::dump_heatmap rudy heatmaps/rudy.csv
EOF
```

### Differential Heatmaps

Noodle 2 generates **differential heatmaps** showing changes between cases:

```
Before ECO:                After ECO:               Difference:
┌─────────────────┐       ┌─────────────────┐       ┌─────────────────┐
│    ░░░░░░░      │       │    ░░░          │       │    ▼▼▼          │
│  ░░████████░░   │  -->  │  ░░██████░░     │  ==>  │  ░░▼▼▼▼▼▼░░     │
│  ░░████████░░   │       │  ░░████░░░░     │       │  ░░▼▼▼▼░░░░     │
│    ░░░░░░░      │       │    ░░░░         │       │    ▼▼▼▼         │
└─────────────────┘       └─────────────────┘       └─────────────────┘
  (congested)               (improved)              (green = better)
```

**Differential Output**:

```
visualizations/
  differential/
    placement_density_diff.csv      # raw diff values
    placement_density_diff.png      # red=worse, green=better
    routing_congestion_diff.csv
    routing_congestion_diff.png
    improvement_summary.json        # quantified improvements per region
```

**Improvement Summary**:

```json
{
  "differential_summary": {
    "placement_density": {
      "improved_bins": 234,
      "worsened_bins": 12,
      "unchanged_bins": 854,
      "net_improvement_pct": 18.5,
      "max_improvement_region": {"x": 150, "y": 200, "delta": -0.35}
    },
    "routing_congestion": {
      "improved_bins": 456,
      "worsened_bins": 23,
      "unchanged_bins": 621,
      "net_improvement_pct": 31.2,
      "resolved_hotspots": [1, 3],
      "new_hotspots": []
    }
  }
}
```

### Critical Path Overlay

Noodle 2 generates heatmaps with **critical path overlays**:

```
┌─────────────────────────────────────────┐
│                                         │
│    ░░░░░░░░░░░░░░░                     │
│  ░░████████████░░░░     ══════════╗    │
│  ░░██████████████░░░    ║ path 1  ║    │
│  ░░████████████░░░░     ════════╦═╝    │
│    ░░░░░░░░░░░░░              ║        │
│         ╔═══════════════════╦═╝        │
│         ║      path 2       ║          │
│         ╚═══════════════════╝          │
│                                         │
└─────────────────────────────────────────┘
  Routing Congestion + Top 5 Critical Paths
```

**Overlay Configuration**:

```yaml
visualization:
  critical_path_overlay:
    enabled: true
    path_count: 10
    color_by: slack              # "slack", "wire_delay", or "cell_delay"
    show_endpoints: true
    show_slack_labels: true
```

### Hotspot Annotation

Heatmaps include **automatic hotspot annotation**:

```
┌─────────────────────────────────────────┐
│                                         │
│    ░░░░░░░░░░░░░░░                     │
│  ░░████████████░░░░   ┌─────────────┐  │
│  ░░██[HS-1]████░░░░   │ HS-1: 0.92  │  │
│  ░░████████████░░░░   │ cause: pins │  │
│    ░░░░░░░░░░░░░      └─────────────┘  │
│                                         │
│         ████████   ┌─────────────┐     │
│       ██[HS-2]██   │ HS-2: 0.78  │     │
│         ████████   │ cause: density│    │
│                    └─────────────┘     │
└─────────────────────────────────────────┘
```

### Pareto Frontier Visualization

For multi-objective optimization, Noodle 2 visualizes the **Pareto frontier**:

```
Hot Ratio
    │
0.4 │     ×
    │   ×   ×
0.3 │ ×       ×
    │   ●───────●     ← Pareto frontier
0.2 │     ●       ●
    │       ●───────●
0.1 │           ●
    │             ●
0.0 └─────────────────────
    -3000  -2000  -1000   0   WNS (ps)

    × = dominated solutions
    ● = Pareto-optimal solutions
    ─ = frontier boundary
```

**Pareto Evolution Animation**:

Noodle 2 generates an animated GIF showing frontier evolution across stages:

```
visualizations/
  pareto_frontier_stage_0.png
  pareto_frontier_stage_1.png
  pareto_frontier_stage_2.png
  pareto_evolution.gif            # animated progression
```

### Improvement Trajectory

Track metric improvement over the study:

```
WNS (ps)
    │
   0│                                    ●────
    │                               ●────
-500│                          ●────
    │                     ●────
-1000│               ●────
    │          ●────
-1500│     ●────
    │●────
-2000│
    └────────────────────────────────────────
         S0    S1    S2    S3    Final

    Stage Progression with Best/Median/Worst
```

### Stage Progression Visualization

```
Stage 0: Exploration          Stage 1: Refinement         Stage 2: Closure
┌─────────────────────┐      ┌─────────────────────┐     ┌─────────────────┐
│ 50 trials           │      │ 30 trials           │     │ 10 trials       │
│ 10 survivors        │ ──▶  │ 3 survivors         │ ──▶ │ 1 winner        │
│                     │      │                     │     │                 │
│ WNS: -2150 → -1200  │      │ WNS: -1200 → -650   │     │ WNS: -650 → -320│
│ Hot:  0.35 →  0.22  │      │ Hot:  0.22 →  0.12  │     │ Hot: 0.12 → 0.08│
└─────────────────────┘      └─────────────────────┘     └─────────────────┘
```

### Visualization Contract

For any stage with visualization enabled, the **minimum required set** is:

| Artifact | Format | Required |
|----------|--------|----------|
| `placement_density` | CSV + PNG | Yes |
| `routing_congestion` | CSV + PNG | Yes |
| `differential` (if parent exists) | PNG | Yes |
| `hotspot_annotations` | JSON + PNG | Yes |
| `critical_path_overlay` | PNG | If timing issue |
| `pareto_frontier` | PNG | If multi-objective |

**Fallback Behavior**:

1. If heatmap generation fails: log `visualization_unavailable`, continue trial
2. If differential fails: skip differential, generate absolute heatmaps only
3. Mark `artifact_index.json` with `visualization_status` field

---

## Ray Dashboard Integration

### Feature: Ray Dashboard as the Operator Console

**Capability**

* Noodle 2 runs on Ray and exposes the **Ray Dashboard** as the default operator UI.
* Operators can observe:
  * cluster health and node status
  * running / completed trial tasks per stage
  * per-stage throughput, failures, and resource utilization

**Single-node execution (MVP)**:

```bash
ray start --head --dashboard-host=0.0.0.0
# Dashboard available at http://localhost:8265
```

### Feature: Trial Artifact Indexing and Deep Links

**Capability**

* Every trial produces an **artifact bundle** in a deterministic location.
* Noodle 2 emits an **artifact index** (`artifact_index.json`) per trial containing:
  * paths to key files (reports, logs, JSON metrics)
  * paths to all visualization artifacts
  * high-level labels (ECO name, stage index, derived case id)
  * visualization status and available heatmaps

**Artifact Index Schema**:

```json
{
  "trial_id": "nangate45_1_5_t003",
  "case": "nangate45_1_5",
  "stage": 1,
  "eco_applied": "resize_critical_drivers",
  "eco_parameters": {"size_multiplier": 1.5, "max_paths": 10},

  "metrics": {
    "wns_ps": -850,
    "tns_ps": -12000,
    "hot_ratio": 0.15
  },

  "metrics_delta": {
    "wns_ps": +350,
    "tns_ps": +8000,
    "hot_ratio": -0.07
  },

  "visualization_status": "complete",
  "visualizations": {
    "heatmaps": {
      "placement_density": "visualizations/heatmaps/placement_density.png",
      "routing_congestion": "visualizations/heatmaps/routing_congestion.png"
    },
    "differential": {
      "placement_density_diff": "visualizations/differential/placement_density_diff.png",
      "routing_congestion_diff": "visualizations/differential/routing_congestion_diff.png"
    },
    "overlays": {
      "critical_paths": "visualizations/overlays/critical_paths.png",
      "hotspots": "visualizations/overlays/hotspots.png"
    }
  },

  "diagnosis": "diagnosis.json",
  "timing_report": "timing_report.txt",
  "congestion_report": "congestion_report.txt",
  "logs": ["logs/openroad.log", "logs/eco_execution.log"],
  "safety_trace": "safety_trace.json"
}
```

---

## Study Comparison

Noodle 2 supports comparing two studies to understand what worked better:

```bash
noodle2 compare --study1 nangate45_v1 --study2 nangate45_v2
```

**Comparison Output**:

```
Study Comparison: nangate45_v1 vs nangate45_v2
══════════════════════════════════════════════════════════════════

Overall Metrics:
─────────────────────────────────────────────────────────────────
Metric                  v1            v2            Delta
─────────────────────────────────────────────────────────────────
Final WNS (ps)          -850          -620          +27.1%  ▲
Final TNS (ps)          -12000        -8500         +29.2%  ▲
Final hot_ratio         0.12          0.08          +33.3%  ▲
Total trials            156           98            -37.2%  ▲
Successful trials       142           89            -37.3%
ECO success rate        68%           75%           +7.0%   ▲
Runtime (min)           45            32            -28.9%  ▲
Stages to converge      4             3             -25.0%  ▲

ECO Effectiveness:
─────────────────────────────────────────────────────────────────
ECO                           v1 success    v2 success    Delta
─────────────────────────────────────────────────────────────────
resize_critical_drivers       72%           85%           +13%  ▲
insert_buffers                65%           70%           +5%   ▲
spread_dense_region           58%           62%           +4%   ▲
reroute_congested_nets        45%           48%           +3%

Key Differences:
─────────────────────────────────────────────────────────────────
• v2 used warm_start from v1 priors (explains higher ECO success)
• v2 used diverse_top_n selection (explains fewer trials needed)
• v2 enabled auto-diagnosis (explains better ECO targeting)

Visualization: comparison_v1_v2/
  pareto_comparison.png
  trajectory_comparison.png
  eco_effectiveness_comparison.png
```

---

## What Noodle 2 Enables

Noodle 2 is designed for:

* unattended, long-running PD experiments
* CI and regression safety checks
* comparative ECO studies
* reproducible demos on open PDKs
* controlled integration of future AI planners

It provides **confidence and control**, not just speed.

---

## Implementation & Platform

### Language & Runtime

**Noodle 2 is implemented in Python.**

* Python 3.10+ is the supported runtime.
* Core responsibilities (policy evaluation, safety logic, telemetry aggregation, case graph management) are pure-Python and deterministic.
* No PD algorithms are reimplemented in Python; all physical design computation is delegated to external tools.

This choice optimizes for:

* rapid iteration and testability
* rich orchestration and policy logic
* straightforward integration with distributed execution frameworks

---

### EDA Tooling & Container Baseline

Noodle 2 executes all physical-design work inside a **standardized Docker container**.

**Primary supported image**:

```
openroad/orfs:latest
```

Alternative images for specific use cases:

```
efabless/openlane:latest          # Sky130-focused workflows
openroad/flow-ubuntu22.04-dev     # Development/debugging
```

Container requirements:

* OpenROAD and OpenSTA on PATH
* `report_checks` for timing analysis
* `global_route -congestion_report_file` for congestion metrics
* Xvfb for headless GUI operations
* Python 3 with matplotlib/numpy for heatmap rendering
* PDK files for target technologies

All tool invocations are executed via Docker wrappers; Noodle 2 does **not** require native OpenROAD installations on the host.

---

### PDK Availability

Noodle 2 supports three reference PDK targets in priority order:

| Priority | PDK | Source | Notes |
|----------|-----|--------|-------|
| 1 | **Nangate45** | OpenROAD-flow-scripts | Fastest iteration, primary bring-up target |
| 2 | **ASAP7** | OpenROAD-flow-scripts | Advanced-node stress testing |
| 3 | **Sky130** | efabless/OpenLane | Production-realistic, fully open |

**PDK paths are resolved inside the container.** No network access is required at runtime.

If a Study requires a modified PDK, it must be introduced via:

* a custom container image, or
* a bind-mounted, versioned PDK override declared in the Study definition

Implicit or ad-hoc PDK replacement is not permitted.

---

### Execution Model

Noodle 2 separates concerns cleanly:

* **Controller layer (Python)** — runs outside the container
* **Trial execution layer (Docker)** — runs OpenROAD/OpenSTA inside the container

Each trial:

* executes in an isolated working directory
* consumes an immutable snapshot
* emits structured artifacts (metrics, logs, reports, visualizations)
* is side-effect free with respect to the base snapshot

This guarantees reproducibility and safe parallelism.

---

### Distributed Orchestration (Ray)

Noodle 2 uses **Ray** as its execution and scheduling substrate.

* Ray provides task-level parallelism for trials within a stage.
* The controller submits trials as Ray tasks with explicit resource requirements.
* Ray's object store is used only for lightweight metadata; all heavy artifacts are written to disk.

**Single-node execution (MVP)**:

For development and demos, single-node Ray execution is the primary mode:

```bash
ray start --head --dashboard-host=0.0.0.0
# Dashboard available at http://localhost:8265
```

**Multi-node support (follow-up)**:

* Noodle 2 supports Ray clusters spanning multiple nodes.
* A shared filesystem (e.g., NFS) is assumed for snapshots and artifacts.
* Multi-node validation is deferred until single-node MVP is stable.

Ray is used strictly as an orchestration engine; it does not participate in policy decisions or safety logic.

---

### Determinism & Reproducibility

Noodle 2 enforces determinism by design:

* no random scheduling decisions in the controller
* stable policy rule ordering
* explicit stage and budget configuration
* fixed OpenROAD random seeds (configurable)
* deterministic survivor selection (with optional diversity randomness seeded)

Given the same Study definition, snapshot, and configuration, Noodle 2 produces equivalent outcomes.

---

## Reference Studies & Validation Gates

### PDK Priority Order

Development and validation follow strict priority order:

1. **Nangate45** — must work first, fastest iteration
2. **ASAP7** — must work second, validates advanced-node handling
3. **Sky130** — must work third, validates production-realistic flows

**Non-negotiable rule**: If any higher-priority PDK regresses, development stops until restored.

---

### Mandatory Smoke Tests

Before any higher-level tests, Noodle 2 must pass these smoke tests:

**Test 1: Nangate45 base case**

```bash
noodle2 run --study nangate45_smoke --eco noop
# Expected: rc=0, wns_ps present, artifacts created, heatmaps generated
```

**Test 2: ASAP7 base case**

```bash
noodle2 run --study asap7_smoke --eco noop
# Expected: rc=0, wns_ps present, artifacts created, heatmaps generated
# Note: ASAP7 workarounds automatically applied
```

**Test 3: Sky130 base case**

```bash
noodle2 run --study sky130_smoke --eco noop
# Expected: rc=0, wns_ps present, artifacts created, heatmaps generated
```

Each test must:

* launch the standard container
* execute a base Study with a no-op ECO
* confirm: `rc == 0`, timing report parsed, artifacts created
* generate all required visualizations
* complete within 5 minutes

**Failure semantics**: Any smoke test failure blocks all downstream development.

---

### Development Validation Ladder

Noodle 2 follows a strict, staged validation ladder:

**Gate 0 — Baseline viability (must pass, else stop)**

For each PDK (in priority order: Nangate45, ASAP7, Sky130):

* Base Case runs inside container
* Produces required reports/artifacts
* Generates all required heatmaps
* Satisfies early-failure and telemetry contracts

**Gate 1 — Full output contract**

Each Base Case produces:

* **Monitoring**: tool return code, timestamps, snapshot hash
* **Timing**: `report_checks` parsed into `wns_ps`, `tns_ps`
* **Congestion**: `global_route -congestion_report_file` parsed into `bins_total`, `bins_hot`, `hot_ratio`
* **Diagnosis**: auto-diagnosis report with issue classification
* **Visualization**: all required heatmaps (placement_density, routing_congestion)
* **Early-failure**: deterministic classification (type/severity/reason)
* **Telemetry**: stage- and case-indexed aggregates
* **Audit**: run legality report + safety trace

**Gate 2 — Controlled failure injection**

Demonstrate correct detection/containment of:

* Worsening slack (more negative WNS)
* Localized congestion hotspots
* Deliberate ECOs that trigger tool errors
* Visualization failures (graceful degradation)

**Gate 3 — Cross-target parity**

Same contracts hold across all three PDKs.

**Gate 4 — Extreme scenarios (demo-grade)**

System handles adversarial conditions while:

* Preserving safety and auditability
* Generating complete visualizations
* Producing compelling before/after comparisons

---

### Required Base Cases

| Case Name | PDK | Design | Purpose |
|-----------|-----|--------|---------|
| `nangate45_base` | Nangate45 | GCD or AES | Fast bring-up, CI smoke |
| `asap7_base` | ASAP7 | GCD | Advanced-node validation |
| `sky130_base` | Sky130 | Ibex | Production-realistic demo |

Each Base Case may be "broken" (negative slack, congestion), but must be **structurally runnable**.

---

### Case Naming Convention

All Cases follow deterministic naming:

```
<pdk>_<stage>_<derived>
```

Examples:

* `nangate45_0_0` — Nangate45 base, stage 0
* `asap7_1_5` — ASAP7 stage 1, derived case 5
* `sky130_2_0` — Sky130 stage 2, first case

---

## Platform-Specific Configuration

### ASAP7 Required Workarounds

ASAP7 requires specific configuration for stable operation:

**1. Explicit routing layers**

```tcl
set_routing_layers -signal metal2-metal9 -clock metal6-metal9
```

**2. Explicit floorplan site**

```tcl
initialize_floorplan \
  -utilization 0.55 \
  -site asap7sc7p5t_28_R_24_NP_162NW_34O
```

**3. Restricted pin placement**

```tcl
place_pins -random \
  -hor_layers {metal4} \
  -ver_layers {metal5}
```

**4. STA-first staging**

* Stage 1: STA-only baseline
* Stage 2+: Congestion analysis enabled after design is stable

**5. Lower utilization target**

* Recommended: 0.50-0.55 (vs 0.60-0.70 for Nangate45)

**Memory anchor**: ASAP7 is "timing-driven, high-metal, low-utilization, STA-first."

---

### Sky130 Configuration

Sky130 uses the **sky130A** variant from the OpenLane ecosystem.

```tcl
# Standard Sky130 configuration
set PDK sky130A
set STD_CELL_LIBRARY sky130_fd_sc_hd
```

---

### Nangate45 Configuration

Nangate45 is the most permissive PDK with minimal special handling.

```tcl
# Standard Nangate45 configuration
# No special workarounds required
```

---

## Extreme Case Generation

### Definition

A case is "extreme" if it reliably produces:

* **Routing pressure**: high overflow / hot-bin ratio from global router
* **Timing pressure**: meaningfully negative WNS with diagnosable cause
* **Determinism**: reproducible failure signature across runs

### Core Techniques

**Congestion amplifiers**:

1. High utilization (0.80-0.88)
2. Routing layer restriction (e.g., metal2-metal4 only)
3. Placement blockages (force routing corridors)
4. Pin crowding (cluster I/O on one edge)

**Timing amplifiers**:

1. Tight clock period
2. Stretched floorplan (wire-dominated paths)
3. Minimal buffering in baseline

### Extreme Case Recipes

**Nangate45 Extreme**:

```yaml
design: AES
utilization: 0.85
core_space: 3
routing_layers: metal2-metal4
pin_placement: north_edge_clustered
blockages:
  - center_horizontal_strip

expected_metrics:
  wns_ps: -2000 to -3000
  hot_ratio: 0.30 to 0.45
```

**ASAP7 Extreme**:

```yaml
design: Ibex
utilization: 0.70
clock_period: 1.0ns
routing_layers: metal2-metal7
blockages:
  - center_corridor

expected_metrics:
  wns_ps: -1500 to -2500
  hot_ratio: 0.25 to 0.40
```

**Sky130 Extreme**:

```yaml
design: Ibex
utilization: 0.75
clock_period: 10ns
pin_placement: north_edge_clustered
blockages:
  - routing_chokepoint

expected_metrics:
  wns_ps: -2500 to -4000
  hot_ratio: 0.30 to 0.50
```

---

## Demo Requirements

### End-to-End Demo Specification

The final deliverable is a set of demo scripts that showcase Noodle 2 fixing extremely broken designs.

**Demo 1: Nangate45 Extreme -> Fixed**

```bash
./demo_nangate45_extreme.sh
```

Demonstrates:

* Starting state: WNS ~ -2000ps, hot_ratio > 0.3
* Auto-diagnosis identifying timing vs congestion issues
* Multi-stage ECO application (3+ stages)
* Early failure detection and containment
* Artifact generation (timing reports, congestion reports)
* Full visualization suite (heatmaps, differentials, overlays)
* Pareto frontier tracking (WNS vs hot_ratio)
* Final state: WNS improved by >50%, hot_ratio < 0.1
* Summary report with improvement trajectory

**Demo 2: ASAP7 Extreme -> Fixed**

```bash
./demo_asap7_extreme.sh
```

Demonstrates:

* ASAP7-specific workarounds automatically applied
* Advanced-node timing challenges
* STA-first staging strategy
* Auto-diagnosis guiding ECO selection
* Complete visualization suite
* Improvement from extreme to acceptable

**Demo 3: Sky130 Extreme -> Fixed**

```bash
./demo_sky130_extreme.sh
```

Demonstrates:

* Production-realistic design (Ibex)
* Full observability stack
* Complete audit trail
* Human approval gate (simulated)
* Final comparison visualizations

### Demo Output Requirements

Each demo must produce:

```
demo_output/
  <pdk>_extreme_demo/
    summary.json                    # Overall improvement metrics
    study_log.txt                   # Human-readable execution log
    study_config.yaml               # Frozen study definition

    diagnosis/                      # Auto-diagnosis reports
      initial_diagnosis.json
      stage_0_diagnosis.json
      stage_1_diagnosis.json
      final_diagnosis.json

    before/                         # Initial state (extreme)
      metrics.json
      diagnosis.json
      timing_report.txt
      congestion_report.txt
      heatmaps/
        placement_density.png
        routing_congestion.png
        rudy.png
      overlays/
        critical_paths.png
        hotspots.png

    stages/                         # Per-stage progression
      stage_0/
        stage_summary.json
        survivors/
          case_0_0/
            metrics.json
            heatmaps/
            differential/           # vs initial
          case_0_1/
            ...
        pareto_frontier.png
      stage_1/
        ...
        differential/               # vs stage_0 best
      stage_N/
        ...

    after/                          # Final state (fixed)
      metrics.json
      diagnosis.json
      timing_report.txt
      congestion_report.txt
      heatmaps/
        placement_density.png
        routing_congestion.png
      overlays/
        critical_paths.png          # should show resolved paths

    comparison/                     # Before/after comparison
      before_after_heatmaps/
        placement_density_before.png
        placement_density_after.png
        placement_density_diff.png
        routing_congestion_before.png
        routing_congestion_after.png
        routing_congestion_diff.png
      timing_improvement.png        # WNS trajectory graph
      congestion_improvement.png    # Hot ratio trajectory graph
      pareto_evolution.gif          # Animated Pareto frontier
      stage_progression.png         # Stage-by-stage summary
      summary_table.txt             # Side-by-side metrics

    eco_analysis/                   # ECO effectiveness analysis
      eco_success_rates.json
      eco_effectiveness_chart.png
      recommended_playbook.json     # learned optimal ECO ordering
```

### Demo Success Criteria

| Metric | Nangate45 | ASAP7 | Sky130 |
|--------|-----------|-------|--------|
| WNS improvement | > 50% | > 40% | > 50% |
| Hot ratio reduction | > 60% | > 50% | > 60% |
| Early failures detected | >= 1 | >= 1 | >= 1 |
| Artifacts complete | 100% | 100% | 100% |
| Heatmaps generated | All | All | All |
| Differential heatmaps | Yes | Yes | Yes |
| Critical path overlay | Yes | Yes | Yes |
| Pareto frontier | Yes | Yes | Yes |
| Improvement trajectory | Yes | Yes | Yes |
| Runtime | < 30 min | < 60 min | < 45 min |

### Demo Visualization Checklist

Each demo must include these visualizations:

**Required Heatmaps** (before, after, and differential):
- [ ] placement_density
- [ ] routing_congestion
- [ ] rudy (before and after only)

**Required Overlays**:
- [ ] critical_paths (top 10 paths on layout)
- [ ] hotspots (annotated congestion regions)

**Required Charts**:
- [ ] WNS improvement trajectory
- [ ] Hot ratio improvement trajectory
- [ ] Pareto frontier (final)
- [ ] Pareto evolution (animated GIF)
- [ ] Stage progression summary
- [ ] ECO success rate chart

**Required Comparisons**:
- [ ] Side-by-side before/after metrics table
- [ ] Differential heatmap summary (bins improved/worsened)

### Demo Execution Policy: NO Mocking, NO Placeholders

**CRITICAL REQUIREMENT**: Demo scripts MUST execute actual study runs with real OpenROAD/OpenSTA trials. This is a non-negotiable requirement.

**Prohibited Practices**:

1. **NO placeholder files**: Never write text strings to `.png` files. All PNG files must be actual images generated by matplotlib or similar visualization libraries.

2. **NO simulated metrics**: Metrics must come from actual OpenROAD execution, not hardcoded values in shell scripts.

3. **NO stub heatmaps**: Heatmap CSVs must contain real grid data exported from OpenROAD's `gui::dump_heatmap` command.

4. **NO mock visualizations**: All visualization artifacts (heatmaps, overlays, charts) must be generated from real data using the visualization modules in `src/visualization/`.

5. **NO fake diagnosis**: Diagnosis reports must be generated by the auto-diagnosis system analyzing real metrics.

**Required Practices**:

1. **Real execution**: Demo scripts must call `run_demo.py` which uses `StudyExecutor` to execute actual trials.

2. **Real visualizations**: All PNG files must be generated using `matplotlib.savefig()` or equivalent, producing valid image data.

3. **Real metrics**: All `metrics.json` files must contain data parsed from actual OpenROAD output.

4. **Real differential heatmaps**: Differential visualizations must be computed from actual before/after heatmap data.

**Verification**:

The `summary.json` for each demo MUST include:
```json
{
  "execution_mode": "ACTUAL_EXECUTION",
  "mocking": false,
  "placeholders": false
}
```

Any demo output with `mocking: true` or `placeholders: true` is considered invalid.

**Rationale**:

The demos are the primary proof point for Noodle 2's capabilities. Placeholder outputs undermine trust and provide no evidence that the system actually works. Real execution with real visualizations demonstrates:

- The full execution pipeline functions correctly
- Visualization infrastructure produces useful output
- Metrics are accurately parsed and reported
- The system handles real OpenROAD behavior

---

## ECO Playbook (Fixing Rules)

Noodle 2 includes a standard ECO playbook for demonstration purposes.

### ECO Categories

**Timing ECOs** (address negative WNS):

| ECO Name | Class | Description | Precondition |
|----------|-------|-------------|--------------|
| `resize_critical_drivers` | `placement_local` | Upsize cells on critical paths | timing issue, cell-dominated |
| `insert_buffers` | `placement_local` | Add buffers to long nets | timing issue, wire-dominated |
| `swap_to_faster_cells` | `placement_local` | Replace cells with faster variants | timing issue |
| `restructure_critical_logic` | `routing_affecting` | Restructure timing-critical cones | severe timing, routing slack |

**Congestion ECOs** (address high hot_ratio):

| ECO Name | Class | Description | Precondition |
|----------|-------|-------------|--------------|
| `spread_dense_region` | `placement_local` | Reduce local placement density | congestion issue, density correlated |
| `reroute_congested_nets` | `routing_affecting` | Find alternative routing paths | congestion issue, specific hotspot |
| `add_routing_blockages` | `routing_affecting` | Guide router away from hotspots | congestion issue |
| `macro_channel_widening` | `global_disruptive` | Increase spacing around macros | congestion near macros |

**Combined ECOs** (address both):

| ECO Name | Class | Description | Precondition |
|----------|-------|-------------|--------------|
| `spread_and_rebuffer` | `placement_local` | Spread + insert buffers | both issues, correlated regions |
| `timing_aware_spreading` | `routing_affecting` | Spread while preserving timing | both issues |

### ECO Application Order

Standard playbook ordering driven by auto-diagnosis:

```
Stage 0: Exploration (diagnosis-driven)
  - Run auto-diagnosis
  - Identify primary issue (timing vs congestion)
  - Apply topology_neutral ECOs to characterize

Stage 1: Targeted Fixes (diagnosis-driven)
  IF primary_issue == "timing":
    IF bottleneck == "wire_dominated":
      - insert_buffers (priority 1)
      - spread_dense_region (priority 2)
    ELSE:  # cell_dominated
      - resize_critical_drivers (priority 1)
      - swap_to_faster_cells (priority 2)

  IF primary_issue == "congestion":
    IF cause == "placement_density":
      - spread_dense_region (priority 1)
    IF cause == "pin_crowding":
      - reroute_congested_nets (priority 1)

Stage 2: Refinement
  - Re-run diagnosis
  - Apply secondary ECOs based on residual issues
  - routing_affecting ECOs if needed

Stage 3: Closure
  - Conservative ECOs only
  - Verify no regressions via differential heatmaps
```

---

## Explicit Non-Goals

Noodle 2 intentionally does **not**:

* invent ECOs (ECOs are provided by humans or external tools)
* replace OpenROAD algorithms
* modify PD tool internals
* rely on opaque optimization or ML
* silently escalate risk

All behavior is declarative, inspectable, and auditable.

---

## Summary

Noodle 2 transforms physical-design experimentation from ad-hoc scripting into a **safety-critical, policy-driven system**.

It enables engineers to:

* explore aggressively when appropriate
* close conservatively when required
* always understand *why* a decision was made
* **see exactly where** improvements happened via comprehensive visualization

Noodle 2 is not an optimizer. It is the control plane that makes optimization trustworthy at scale.

The demo deliverables prove this by taking three extremely broken designs (Nangate45, ASAP7, Sky130) and systematically fixing them through:

* **Auto-diagnosis** identifying root causes
* **Multi-stage ECO application** with intelligent selection
* **Full observability** including heatmaps, overlays, and differentials
* **Pareto optimization** balancing timing and congestion
* **Early failure detection** and graceful containment
* **Auditable improvement metrics** with compelling visualizations

The result: broken designs become fixed designs, with visual proof at every step.

---

## Cross-Project Prior Learning

### Beyond Single-Study Priors

While warm-start enables prior transfer within a project (across studies), **cross-project prior learning** enables transfer across entirely different projects, designs, and even organizations.

### Prior Repository

```yaml
prior_repository:
  location: ~/.noodle2/priors/          # global prior storage
  format: sqlite                        # or json, parquet

  # Project-level priors
  projects:
    - name: project_alpha
      studies: [nangate45_v1, nangate45_v2, asap7_timing_fix]
      weight: 1.0

    - name: project_beta
      studies: [sky130_congestion_study]
      weight: 0.7

  # Cross-project aggregation
  aggregation:
    method: weighted_average            # or "best_performer", "ensemble"
    min_samples: 5                      # minimum trials before trusting
    decay_by_age: true
    decay_half_life_days: 30
```

### Prior Export/Import

```bash
# Export priors from completed study
noodle2 priors export --study nangate45_v3 --output priors_nangate45_v3.json

# Import priors into new project
noodle2 priors import --file priors_nangate45_v3.json --weight 0.8

# Share priors across team (optional)
noodle2 priors publish --study nangate45_v3 --repository team_priors
noodle2 priors fetch --repository team_priors --eco resize_critical_drivers
```

### Cross-Project Prior Schema

```json
{
  "prior_version": "1.0",
  "source_project": "project_alpha",
  "source_study": "nangate45_v3",
  "export_timestamp": "2026-01-12T15:30:00Z",

  "eco_priors": {
    "resize_critical_drivers": {
      "total_applications": 47,
      "success_rate": 0.78,
      "average_wns_improvement_ps": 245,
      "average_runtime_seconds": 34,
      "prior_state": "trusted",
      "confidence": 0.92,

      "context_breakdown": {
        "wire_dominated": {"success_rate": 0.65, "samples": 18},
        "cell_dominated": {"success_rate": 0.89, "samples": 29}
      },

      "parameter_recommendations": {
        "size_multiplier": {"optimal_range": [1.3, 1.6], "avoid": [2.0]},
        "max_paths": {"optimal_range": [15, 25]}
      }
    },

    "insert_buffers": {
      "total_applications": 62,
      "success_rate": 0.71,
      "average_wns_improvement_ps": 312,
      "prior_state": "trusted",
      "confidence": 0.88
    }
  },

  "pdk_compatibility": ["nangate45", "asap7"],
  "design_complexity_range": {"min_cells": 1000, "max_cells": 100000}
}
```

### Learning from Failures

Cross-project priors also track **anti-patterns**:

```json
{
  "anti_patterns": {
    "resize_on_congested_region": {
      "pattern": "resize_critical_drivers when hot_ratio > 0.4",
      "failure_rate": 0.73,
      "typical_outcome": "timing improves but congestion explodes",
      "recommendation": "apply spread_dense_region first"
    },

    "aggressive_buffering_on_asap7": {
      "pattern": "insert_buffers with max_buffers > 100 on ASAP7",
      "failure_rate": 0.81,
      "typical_outcome": "placement density violations",
      "recommendation": "limit to max_buffers <= 50 for ASAP7"
    }
  }
}
```

---

## Doomed Case Early Termination

### Definition

A case is **doomed** when continued execution is statistically unlikely to produce acceptable results. Early termination saves compute and prevents wasted effort.

### Doom Detection Criteria

```yaml
doom_detection:
  enabled: true

  # Metric-based doom
  metric_thresholds:
    wns_ps_hopeless: -10000           # WNS worse than -10ns is hopeless
    hot_ratio_hopeless: 0.8           # hot_ratio > 0.8 is unrecoverable
    tns_ps_hopeless: -500000          # TNS worse than -500ns

  # Trajectory-based doom
  trajectory_analysis:
    enabled: true
    min_stages_for_analysis: 2
    regression_threshold: 0.1         # 10% regression triggers doom check
    stagnation_threshold: 0.02        # <2% improvement is stagnation
    consecutive_stagnation: 3         # 3 stages of stagnation = doomed

  # ECO exhaustion doom
  eco_exhaustion:
    enabled: true
    all_ecos_failed: true             # doom if all ECOs have failed
    all_ecos_suspicious: true         # doom if all ECOs are suspicious

  # Statistical doom
  statistical_doom:
    enabled: true
    min_trials_for_analysis: 10
    success_rate_threshold: 0.1       # <10% success rate = doom
    improvement_percentile: 0.05      # bottom 5% of expected improvement
```

### Doom Classification

| Doom Type | Trigger | Action |
|-----------|---------|--------|
| `metric_hopeless` | Metrics exceed hopeless thresholds | Terminate immediately |
| `trajectory_doom` | Consistent regression or stagnation | Terminate after 2 more trials |
| `eco_exhaustion` | All applicable ECOs failed/suspicious | Terminate, report ECO gaps |
| `statistical_doom` | Success rate below threshold | Terminate, flag for review |
| `rail_doom` | Rail violation with no recovery path | Terminate immediately |

### Doom Response

```json
{
  "doom_classification": {
    "case": "nangate45_2_7",
    "doom_type": "trajectory_doom",
    "trigger": "3 consecutive stages with <2% WNS improvement",

    "evidence": {
      "stage_0_improvement": 15.2,
      "stage_1_improvement": 1.8,
      "stage_2_improvement": 0.9,
      "stage_3_improvement": 0.4
    },

    "action": "terminated",
    "compute_saved_trials": 24,
    "recommendation": "Design may need architectural changes beyond ECO scope"
  }
}
```

---

## Immutable Design Rules (What Cannot Change)

### Protected Elements

Noodle 2 enforces **immutable design rules** that specify elements that must NEVER be modified by ECOs.

```yaml
immutable_rules:
  # Structural immutables
  structural:
    - macro_locations: true           # never move macros
    - io_pin_positions: true          # never move I/O pins
    - power_grid: true                # never modify PG mesh
    - clock_tree_topology: true       # never restructure clock tree

  # Timing immutables
  timing:
    - clock_definitions: true         # never change clock constraints
    - false_paths: true               # never modify false path exceptions
    - multicycle_paths: true          # preserve multicycle definitions

  # Net immutables
  nets:
    protected_nets:
      - "clk*"                        # all clock nets
      - "reset*"                      # all reset nets
      - "vdd*"                        # power nets
      - "vss*"                        # ground nets

  # Cell immutables
  cells:
    protected_cells:
      - pattern: "*_macro"            # all macros
      - pattern: "pad_*"              # all pads
      - instance: "pll_inst"          # specific instances

    size_locked:
      - pattern: "clkbuf_*"           # clock buffers size-locked
```

### Mutation Permissions Matrix

| Element Type | topology_neutral | placement_local | routing_affecting | global_disruptive |
|--------------|------------------|-----------------|-------------------|-------------------|
| Standard cells | ❌ | ✅ resize/move | ✅ restructure | ✅ all |
| Buffers | ❌ | ✅ insert/remove | ✅ | ✅ |
| Clock buffers | ❌ | ❌ | ❌ | ⚠️ with approval |
| Macros | ❌ | ❌ | ❌ | ⚠️ with approval |
| I/O pads | ❌ | ❌ | ❌ | ❌ never |
| Power grid | ❌ | ❌ | ❌ | ❌ never |

### Violation Detection

```json
{
  "immutable_violation": {
    "eco": "aggressive_placement_fix",
    "attempted_mutation": "move macro M1 by (50, 100) um",
    "rule_violated": "macro_locations",
    "action": "ECO_BLOCKED",
    "severity": "critical",
    "message": "Macro M1 is protected. ECO cannot modify macro locations."
  }
}
```

---

## Ray Dashboard Requirements

### Mandatory Dashboard for Demos

**CRITICAL**: The Ray Dashboard MUST be started and accessible during all demo executions.

```yaml
ray_dashboard:
  mandatory: true
  host: "0.0.0.0"                     # accessible from any interface
  port: 8265

  startup:
    auto_start: true                  # automatically start with demo
    wait_for_ready: true              # block until dashboard is accessible
    timeout_seconds: 30

  display:
    print_url: true                   # print dashboard URL at startup
    open_browser: false               # don't auto-open (headless compatible)
```

### Demo Startup Sequence

```python
# Required startup sequence for demos
def start_demo():
    # 1. Initialize Ray with dashboard
    ray.init(
        dashboard_host="0.0.0.0",
        dashboard_port=8265,
        include_dashboard=True,
    )

    # 2. Wait for dashboard to be ready
    dashboard_url = f"http://localhost:8265"
    wait_for_dashboard(dashboard_url, timeout=30)

    # 3. Print dashboard URL prominently
    print("=" * 60)
    print(f"RAY DASHBOARD: {dashboard_url}")
    print("Open this URL to monitor trial execution in real-time")
    print("=" * 60)

    # 4. Execute study
    executor = StudyExecutor(config, use_ray=True)
    results = executor.run()

    # 5. Shutdown Ray cleanly
    ray.shutdown()
```

### Dashboard Observability Contract

During demo execution, the Ray Dashboard MUST show:

| Metric | Location | Update Frequency |
|--------|----------|------------------|
| Running trials | Jobs tab | Real-time |
| Completed trials | Jobs tab | Real-time |
| Failed trials | Jobs tab | Real-time |
| CPU/Memory usage | Cluster tab | 1 second |
| Trial logs | Logs tab | Real-time |

---

## Implementation Status & Production Gaps

### Current Implementation Status

This section tracks what is **actually implemented** vs **stubbed/placeholder**.

#### ✅ Implemented (Working)

| Component | Location | Status |
|-----------|----------|--------|
| Study configuration | `src/controller/study_config.py` | Complete |
| Stage execution | `src/controller/executor.py` | Complete |
| Docker trial runner | `src/trial_runner/docker_runner.py` | Complete |
| Ray parallel execution | `src/trial_runner/ray_executor.py` | Complete |
| Safety domain enforcement | `src/controller/safety_domain.py` | Complete |
| Basic metrics parsing | `src/parsers/timing_metrics.py` | Complete |
| Checkpoint/resume | `src/controller/checkpoint.py` | Complete |

#### 🟡 Partial (Needs Work)

| Component | Location | Gap |
|-----------|----------|-----|
| ECO execution | `src/controller/eco.py` | Only NoOp and BufferInsertion; missing resize, swap |
| Visualization | `src/visualization/` | Creates charts but some are placeholder text files |
| Auto-diagnosis | `src/analysis/diagnosis.py` | Uses hardcoded heuristics, not real path analysis |
| Prior learning | `src/controller/eco.py` | Basic priors exist; no cross-project support |

#### ❌ Stubbed/Missing

| Component | Location | Issue |
|-----------|----------|-------|
| Cell resize ECO | Not implemented | Critical for real optimization |
| Cell swap ECO | Not implemented | Critical for timing fixes |
| Real heatmap generation | `src/visualization/heatmap.py` | Writes placeholder text to .png files |
| Real path breakdown | `src/analysis/diagnosis.py` | Uses 75%/25% hardcoded split |
| Cross-project priors | Not implemented | No prior repository |
| Doom detection | Not implemented | No intelligent early termination |
| CLI commands | `src/cli.py` | 5 commands are stubs |

### Production Readiness Gaps (from Audit)

#### Critical (Demo-Breaking)

1. **No real ECO execution** - Trials run STA on same snapshot, no changes applied
2. **Missing CellResizeECO** - OpenROAD `resize` command not integrated
3. **Missing CellSwapECO** - Cannot swap to faster/slower cell variants
4. **Synthetic metrics** - Congestion and heatmap data is estimated, not real
5. **Placeholder PNGs** - Some visualization writes text to .png files
6. **No timing violations** - Demo designs have positive slack, nothing to fix

#### High Priority

7. **20+ hardcoded thresholds** in `diagnosis.py` - should be configurable
8. **Fake ECO names** in executor - tracks "trial_eco_N" not actual ECO
9. **Trial sorting by return code** - should sort by WNS improvement
10. **No real path breakdown** - wire vs cell delay is guessed
11. **Missing buffer removal** - can insert but not remove buffers
12. **Missing Vt swap** - no high-Vt to low-Vt optimization
13. **FULL_ROUTE mode stub** - delegates to STA mode

#### Medium Priority

14. **5 CLI commands are stubs** - run_study, validate, show, export, init
15. **Incomplete trajectory visualization** - shows final only, not progression
16. **10 silent error handlers** - bare `pass` statements swallow errors
17. **No runtime prior updates** - priors computed offline only
18. **Guardrails not intelligent** - thresholds don't scale by stage

---

## Implementation Roadmap

### Phase 1: Real ECO Execution (Critical Path)

**Goal**: Make trials apply actual ECOs that modify the design and produce varying results.

#### 1.1 Implement CellResizeECO

```python
class CellResizeECO(ECO):
    """Resize cells on critical paths using OpenROAD repair_design."""

    eco_class = ECOClass.PLACEMENT_LOCAL

    def generate_tcl(self, params: dict) -> str:
        return f"""
        # Resize cells on critical paths
        set_wire_rc -layer metal3
        estimate_parasitics -placement

        # Target critical paths
        repair_design -max_wire_length {params.get('max_wire_length', 100)} \\
                      -buffer_cell {params.get('buffer_cell', 'BUF_X4')} \\
                      -max_utilization {params.get('max_util', 0.8)}

        # Report changes
        report_design_area
        """
```

#### 1.2 Create Broken Design Snapshots

Generate designs with actual timing violations:

```tcl
# Create timing-violated snapshot
create_clock -name clk -period 0.3 [get_ports clk]  # Aggressive 0.3ns
# This will produce WNS ~ -2000ps on GCD design
```

#### 1.3 Trial Differentiation

Each trial MUST apply different ECO parameters:

```python
def generate_trial_configs(base_config, num_trials):
    configs = []
    for i in range(num_trials):
        config = base_config.copy()
        # Vary parameters
        config['eco_params'] = {
            'size_multiplier': 1.2 + (i * 0.1),  # 1.2, 1.3, 1.4, ...
            'max_paths': 10 + (i * 5),            # 10, 15, 20, ...
        }
        configs.append(config)
    return configs
```

### Phase 2: Real Metrics & Visualization

**Goal**: Parse actual OpenROAD output and generate real visualizations.

#### 2.1 Real Path Breakdown

```tcl
# Extract actual wire vs cell delay
report_checks -path_delay max -fields {fanout cap slew cell net} -digits 4

# Parse output to get:
# - cell_delay: sum of cell delays
# - wire_delay: sum of net delays
# - wire_dominated: wire_delay > 0.65 * total
```

#### 2.2 Real Heatmap Generation

```python
def generate_real_heatmap(csv_path: Path, output_png: Path):
    """Generate actual PNG from OpenROAD heatmap CSV."""
    import matplotlib.pyplot as plt
    import numpy as np

    # Load real data from OpenROAD export
    data = np.loadtxt(csv_path, delimiter=',')

    # Create actual visualization
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(data, cmap='hot', interpolation='nearest')
    plt.colorbar(im, ax=ax, label='Density')
    ax.set_title('Placement Density Heatmap')

    # Save as real PNG
    plt.savefig(output_png, dpi=150, bbox_inches='tight')
    plt.close()
```

#### 2.3 Real Congestion Parsing

```tcl
# Run global route and extract real congestion
global_route -congestion_report_file congestion.rpt

# Parse congestion.rpt for:
# - bins_total: total routing bins
# - bins_hot: bins with overflow
# - hot_ratio: bins_hot / bins_total
# - layer_breakdown: per-layer overflow
```

### Phase 3: Doom Detection & Smart Termination

**Goal**: Implement intelligent early termination of hopeless cases.

#### 3.1 Doom Detector

```python
class DoomDetector:
    def __init__(self, config: DoomConfig):
        self.config = config
        self.history = []

    def check_doom(self, case: Case, metrics: Metrics) -> Optional[DoomClassification]:
        # Check metric thresholds
        if metrics.wns_ps < self.config.wns_ps_hopeless:
            return DoomClassification(
                doom_type="metric_hopeless",
                trigger=f"WNS {metrics.wns_ps} < threshold {self.config.wns_ps_hopeless}",
                action="terminate_immediately"
            )

        # Check trajectory
        if len(self.history) >= 3:
            improvements = [h.improvement_pct for h in self.history[-3:]]
            if all(imp < self.config.stagnation_threshold for imp in improvements):
                return DoomClassification(
                    doom_type="trajectory_doom",
                    trigger="3 consecutive stages with stagnation",
                    action="terminate"
                )

        return None  # Not doomed
```

### Phase 4: Cross-Project Priors

**Goal**: Enable prior learning across projects.

#### 4.1 Prior Repository

```python
class PriorRepository:
    def __init__(self, path: Path = Path.home() / ".noodle2" / "priors"):
        self.path = path
        self.db = sqlite3.connect(path / "priors.db")

    def export_priors(self, study: Study) -> dict:
        """Export priors from completed study."""
        return {
            "source_study": study.name,
            "eco_priors": study.get_eco_priors(),
            "anti_patterns": study.get_anti_patterns(),
            "export_timestamp": datetime.now().isoformat()
        }

    def import_priors(self, priors: dict, weight: float = 1.0):
        """Import priors into repository."""
        for eco_name, prior in priors["eco_priors"].items():
            self.merge_prior(eco_name, prior, weight)

    def get_aggregated_prior(self, eco_name: str) -> ECOPrior:
        """Get weighted aggregate of all priors for an ECO."""
        priors = self.db.query_priors(eco_name)
        return self.weighted_average(priors)
```

### Phase 5: Polish & CLI

**Goal**: Complete CLI, fix error handling, make thresholds configurable.

#### 5.1 Complete CLI Commands

```python
@cli.command()
def run_study(study_path: Path, use_ray: bool = True):
    """Run a complete study (NOT a stub)."""
    config = StudyConfig.load(study_path)
    executor = StudyExecutor(config, use_ray=use_ray)
    results = executor.run()
    print_summary(results)

@cli.command()
def validate(study_path: Path):
    """Validate study configuration (NOT a stub)."""
    config = StudyConfig.load(study_path)
    issues = config.validate()
    if issues:
        for issue in issues:
            print(f"ERROR: {issue}")
        sys.exit(1)
    print("Configuration valid")
```

#### 5.2 Configurable Thresholds

```yaml
# All thresholds externalized to config
diagnosis:
  thresholds:
    wire_delay_threshold: 0.65        # was hardcoded
    hotspot_threshold: 0.7            # was hardcoded
    wns_wire_dominated: 1500          # was hardcoded magic number

  correlations:
    method: compute                   # "compute" or "hardcoded"
    # If hardcoded:
    placement_density_high: 0.72
    placement_density_low: 0.45
```

---

## Extreme Demo Specifications

### Nangate45 Extreme Demo

**Starting State** (intentionally broken):

```yaml
design: GCD
clock_period: 0.3ns                   # Aggressive - will cause violations
expected_wns_ps: -2000 to -3000
expected_hot_ratio: 0.30 to 0.45
```

**Demo Flow**:

1. Launch with Ray dashboard: `python run_demo.py nangate45 --use-ray`
2. Dashboard URL printed: `http://localhost:8265`
3. Stage 0: Exploration (15 trials, parallel)
   - Auto-diagnosis identifies wire-dominated timing
   - BufferInsertionECO and CellResizeECO applied
   - 2-3 trials terminated early (doom detection)
4. Stage 1: Refinement (10 trials)
   - Surviving cases improved
   - Differential heatmaps show improvement
5. Stage 2: Closure (8 trials)
   - Final optimization
   - Rails checked, no violations
6. Output: Full visualization suite + metrics

**Success Criteria**:

- WNS improved by >50%
- hot_ratio < 0.12
- At least 1 doomed case terminated early
- All PNGs are real images (not placeholders)
- Ray dashboard showed real-time progress

### ASAP7 Extreme Demo

**Starting State**:

```yaml
design: GCD
clock_period: 0.8ns                   # Aggressive for 7nm
expected_wns_ps: -1500 to -2500
expected_hot_ratio: 0.25 to 0.40
```

**Special Handling**:

- ASAP7 workarounds automatically applied
- STA-first staging (congestion disabled in Stage 0)
- Lower utilization targets

### Sky130 Extreme Demo

**Starting State**:

```yaml
design: GCD
clock_period: 8ns                     # Aggressive for 130nm
expected_wns_ps: -2500 to -4000
expected_hot_ratio: 0.30 to 0.50
```

**Special Handling**:

- Production-realistic PDK
- Full audit trail generation
- Human approval gate (simulated with 1-second timeout)

---

## Verification Checklist

Before considering the implementation complete, ALL of the following must be true:

### ECO Execution

- [ ] CellResizeECO implemented with real OpenROAD `resize` command
- [ ] CellSwapECO implemented for Vt optimization
- [ ] BufferInsertionECO produces measurable WNS change
- [ ] Each trial produces DIFFERENT metrics (not all identical)
- [ ] ECO TCL scripts are syntactically valid OpenROAD commands

### Metrics & Visualization

- [ ] All PNG files open as valid images in image viewers
- [ ] Heatmap PNGs contain actual colored visualizations (not text)
- [ ] Differential heatmaps show red/green for worse/better
- [ ] WNS trajectory shows actual stage-by-stage progression
- [ ] Pareto frontier shows actual dominated/non-dominated points

### Rails & Safety

- [ ] Abort rails terminate trials that exceed thresholds
- [ ] Stage rails stop stages with >80% failure rate
- [ ] Study rails halt studies after catastrophic failures
- [ ] Immutable rules prevent modification of protected elements
- [ ] Legality check rejects illegal ECO/domain combinations

### Doom Detection

- [ ] Metric-hopeless cases terminated immediately
- [ ] Trajectory-doom cases terminated after evidence
- [ ] ECO-exhaustion detected and reported
- [ ] Compute savings logged for terminated cases

### Priors

- [ ] Priors updated after each trial
- [ ] Suspicious ECOs deprioritized
- [ ] Warm-start loads priors from previous studies
- [ ] Cross-project prior export/import works

### Ray Integration

- [ ] Ray dashboard starts automatically with demos
- [ ] Dashboard URL printed prominently at startup
- [ ] Trials visible in Ray Jobs tab
- [ ] Parallel execution faster than sequential
- [ ] Clean shutdown without orphan processes

### Demo Quality

- [ ] Nangate45 demo completes with >50% WNS improvement
- [ ] ASAP7 demo completes with >40% WNS improvement
- [ ] Sky130 demo completes with >50% WNS improvement
- [ ] Each demo shows at least 1 early-terminated case
- [ ] All visualization artifacts present and valid
- [ ] Summary JSON shows `mocking: false, placeholders: false`

---

## ECO Catalog with Real OpenROAD TCL

### Critical: These are REAL OpenROAD commands, not pseudo-code

#### 1. CellResizeECO (repair_design)

```tcl
# CellResizeECO - Uses OpenROAD's repair_design for timing optimization
# This is the PRIMARY ECO for timing improvement

proc run_cell_resize_eco {params} {
    # Extract parameters
    set max_wire_length [dict get $params max_wire_length]
    set buffer_cell [dict get $params buffer_cell]
    set max_util [dict get $params max_utilization]

    # Set wire RC for parasitic estimation
    set_wire_rc -signal -layer metal3

    # Estimate parasitics based on placement
    estimate_parasitics -placement

    # Run repair_design - THIS ACTUALLY MODIFIES THE DESIGN
    # It will:
    # - Insert buffers on long wires
    # - Resize gates for timing
    # - Fix max capacitance violations
    # - Fix max slew violations
    repair_design \
        -max_wire_length $max_wire_length \
        -buffer_cell $buffer_cell \
        -max_utilization $max_util

    # Re-run STA after changes
    report_checks -path_delay max -fields {slew cap input_pin net} -digits 4

    # Report what changed
    report_design_area
}
```

**Parameter Ranges:**
- `max_wire_length`: 50-200 (microns) - lower = more buffers
- `buffer_cell`: BUF_X1, BUF_X2, BUF_X4, BUF_X8 - larger = faster but more area
- `max_utilization`: 0.7-0.9 - higher = more aggressive

#### 2. BufferInsertionECO (repair_timing)

```tcl
# BufferInsertionECO - Targeted buffer insertion for setup violations
proc run_buffer_insertion_eco {params} {
    set slack_margin [dict get $params slack_margin]
    set max_buffer_percent [dict get $params max_buffer_percent]

    # Estimate parasitics
    set_wire_rc -signal -layer metal3
    estimate_parasitics -placement

    # Repair setup timing violations
    # This inserts buffers specifically to fix setup violations
    repair_timing \
        -setup \
        -slack_margin $slack_margin \
        -max_buffer_percent $max_buffer_percent

    # Report timing after repair
    report_worst_slack -max
    report_tns -digits 3
}
```

**Parameter Ranges:**
- `slack_margin`: 0-100 (ps) - extra margin beyond zero slack
- `max_buffer_percent`: 5-30 (%) - max area increase from buffers

#### 3. GateCloningECO (repair_timing with cloning)

```tcl
# GateCloningECO - Clone high-fanout gates to reduce load
proc run_gate_cloning_eco {params} {
    set max_fanout [dict get $params max_fanout]

    estimate_parasitics -placement

    # Repair high fanout by cloning gates
    repair_timing \
        -setup \
        -max_fanout $max_fanout

    report_checks -path_delay max
}
```

#### 4. HoldFixECO (repair_timing -hold)

```tcl
# HoldFixECO - Fix hold violations by inserting delay buffers
proc run_hold_fix_eco {params} {
    set slack_margin [dict get $params hold_slack_margin]
    set max_buffer_percent [dict get $params max_buffer_percent]

    estimate_parasitics -placement

    # Fix hold violations
    repair_timing \
        -hold \
        -slack_margin $slack_margin \
        -max_buffer_percent $max_buffer_percent

    report_worst_slack -min
}
```

#### 5. PlacementRefinementECO (detailed_placement)

```tcl
# PlacementRefinementECO - Re-run detailed placement for better timing
proc run_placement_refinement_eco {params} {
    # Re-run detailed placement with timing-driven mode
    detailed_placement

    # Optimize placement for timing
    optimize_mirroring

    # Report new timing
    estimate_parasitics -placement
    report_checks -path_delay max
}
```

---

## Extreme Snapshot Generation

### How to Create Broken Designs with Timing Violations

The demos require designs that START with timing violations. Here's how to generate them:

#### Nangate45 Extreme Snapshot Generator

```tcl
# generate_nangate45_extreme.tcl
# Run this ONCE to create the extreme snapshot

# Load PDK
set platform_dir "/OpenROAD-flow-scripts/flow/platforms/nangate45"
read_lef $platform_dir/lef/NangateOpenCellLibrary.tech.lef
read_lef $platform_dir/lef/NangateOpenCellLibrary.macro.mod.lef
read_liberty $platform_dir/lib/NangateOpenCellLibrary_typical.lib

# Read the GCD design (synthesized netlist)
read_verilog $platform_dir/../../designs/nangate45/gcd/gcd.v
link_design gcd

# Create AGGRESSIVE clock constraint to induce violations
# Normal GCD runs at 2ns, we'll use 0.3ns (3.3GHz - impossible)
create_clock -name clk -period 0.3 [get_ports clk]
set_input_delay -clock clk 0.05 [all_inputs]
set_output_delay -clock clk 0.05 [all_outputs]

# Initialize floorplan with high utilization (causes congestion)
initialize_floorplan \
    -utilization 0.85 \
    -aspect_ratio 1.0 \
    -core_space 2

# Place pins
place_pins -random -hor_layers metal3 -ver_layers metal2

# Global placement (no timing optimization)
global_placement -skip_initial_place

# Detailed placement
detailed_placement

# Now the design has:
# - Aggressive clock (will cause negative WNS)
# - High utilization (will cause congestion)
# - No buffer insertion (wire delays are high)

# Report the "broken" state
report_checks -path_delay max
set wns [sta::worst_slack -max]
puts "EXTREME SNAPSHOT WNS: $wns ns"
puts "This should be approximately -2ns to -3ns"

# Save the snapshot
write_db gcd_extreme.odb

puts "Extreme snapshot saved to gcd_extreme.odb"
```

#### Expected Extreme Metrics

| PDK | Clock Period | Expected WNS | Expected hot_ratio |
|-----|--------------|--------------|-------------------|
| Nangate45 | 0.3ns | -2000 to -3000 ps | 0.30-0.45 |
| ASAP7 | 0.8ns | -1500 to -2500 ps | 0.25-0.40 |
| Sky130 | 8.0ns | -2500 to -4000 ps | 0.30-0.50 |

---

## ECO Selection Algorithm

### How Auto-Diagnosis Drives ECO Selection

```python
class ECOSelector:
    """Selects ECOs based on diagnosis results and priors."""

    def select_ecos(self, diagnosis: DiagnosisResult, priors: PriorRepository) -> List[ECOConfig]:
        """
        Algorithm:
        1. Identify primary issue (timing vs congestion)
        2. Classify bottleneck (wire vs cell dominated)
        3. Filter ECOs by preconditions
        4. Rank by prior effectiveness
        5. Exclude suspicious ECOs in guarded/locked domains
        """
        ecos = []

        # Step 1: Primary issue
        if diagnosis.primary_issue == "timing":
            if diagnosis.timing.dominant_issue == "wire_dominated":
                # Wire-dominated: buffers help most
                candidates = [
                    ("buffer_insertion", {"slack_margin": 50, "max_buffer_percent": 20}),
                    ("cell_resize", {"max_wire_length": 100, "buffer_cell": "BUF_X4"}),
                ]
            else:
                # Cell-dominated: resize helps most
                candidates = [
                    ("cell_resize", {"max_wire_length": 150, "buffer_cell": "BUF_X2"}),
                    ("gate_cloning", {"max_fanout": 16}),
                ]
        else:  # congestion
            candidates = [
                ("placement_refinement", {}),
                # Note: routing ECOs would go here but require full route
            ]

        # Step 2: Filter by priors
        for eco_name, params in candidates:
            prior = priors.get_prior(eco_name)

            # Skip suspicious ECOs in non-sandbox domains
            if prior.state == "suspicious" and self.safety_domain != "sandbox":
                continue

            # Adjust parameters based on prior recommendations
            if prior.parameter_recommendations:
                params = self._apply_recommendations(params, prior.parameter_recommendations)

            ecos.append(ECOConfig(name=eco_name, params=params, prior=prior))

        # Step 3: Sort by expected effectiveness
        ecos.sort(key=lambda e: e.prior.average_wns_improvement_ps, reverse=True)

        return ecos
```

---

## Audit Trail Specification

### safety_trace.json Format

Every trial MUST produce a `safety_trace.json` with this exact format:

```json
{
  "trial_id": "nangate45_extreme_0_5",
  "timestamp": "2026-01-12T21:30:45.123Z",

  "safety_domain": "guarded",
  "eco_class_requested": "placement_local",
  "eco_class_permitted": true,

  "pre_execution_checks": {
    "legality_check": "passed",
    "snapshot_hash_verified": true,
    "prior_state": "trusted",
    "resource_budget_available": true
  },

  "execution_trace": [
    {
      "step": 1,
      "action": "load_snapshot",
      "timestamp": "2026-01-12T21:30:45.200Z",
      "result": "success"
    },
    {
      "step": 2,
      "action": "apply_eco",
      "eco_name": "cell_resize",
      "eco_params": {"max_wire_length": 100, "buffer_cell": "BUF_X4"},
      "timestamp": "2026-01-12T21:30:46.500Z",
      "result": "success",
      "cells_modified": 47,
      "buffers_inserted": 23
    },
    {
      "step": 3,
      "action": "run_sta",
      "timestamp": "2026-01-12T21:30:48.100Z",
      "result": "success"
    }
  ],

  "rail_checks": {
    "abort_rail_wns": {
      "threshold": -10000,
      "actual": -1850,
      "status": "within_limit"
    },
    "abort_rail_timeout": {
      "threshold_seconds": 600,
      "actual_seconds": 12.5,
      "status": "within_limit"
    }
  },

  "metrics_before": {
    "wns_ps": -2850,
    "tns_ps": -45000,
    "hot_ratio": 0.35
  },

  "metrics_after": {
    "wns_ps": -1850,
    "tns_ps": -28000,
    "hot_ratio": 0.32
  },

  "metrics_delta": {
    "wns_ps": 1000,
    "tns_ps": 17000,
    "hot_ratio": -0.03,
    "wns_improvement_pct": 35.1
  },

  "classification": {
    "outcome": "success",
    "failure_type": null,
    "severity": null,
    "prior_update": "trusted"
  },

  "audit_summary": {
    "compute_time_seconds": 12.5,
    "artifacts_generated": [
      "metrics.json",
      "timing_report.txt",
      "safety_trace.json",
      "heatmaps/placement_density.png"
    ],
    "immutable_violations": [],
    "warnings": []
  }
}
```

---

## Prior Update Algorithm

### How Priors Are Updated After Each Trial

```python
class PriorUpdater:
    """Updates ECO priors based on trial outcomes."""

    def update_prior(self, eco_name: str, trial_result: TrialResult) -> None:
        """
        Update rules:

        1. SUCCESS (WNS improved):
           - Increment success_count
           - Update average_wns_improvement
           - If 3+ consecutive successes: state -> "trusted"

        2. REGRESSION (WNS worsened):
           - Increment failure_count
           - If 2+ consecutive failures: state -> "mixed"
           - If 3+ consecutive failures: state -> "suspicious"

        3. CATASTROPHIC (tool crash, timeout):
           - state -> "suspicious" immediately
           - Record failure context for anti-pattern learning
        """
        prior = self.get_prior(eco_name)

        prior.total_applications += 1

        if trial_result.wns_improved:
            prior.success_count += 1
            prior.consecutive_successes += 1
            prior.consecutive_failures = 0

            # Update running average
            prior.average_wns_improvement_ps = (
                (prior.average_wns_improvement_ps * (prior.success_count - 1) +
                 trial_result.wns_delta_ps) / prior.success_count
            )

            # Promote to trusted after consistent success
            if prior.consecutive_successes >= 3 and prior.state != "trusted":
                prior.state = "trusted"
                prior.confidence = min(0.95, prior.confidence + 0.1)

        elif trial_result.is_catastrophic:
            prior.state = "suspicious"
            prior.consecutive_failures += 1
            prior.consecutive_successes = 0
            prior.confidence = max(0.1, prior.confidence - 0.3)

            # Record anti-pattern
            self.record_anti_pattern(eco_name, trial_result)

        else:  # Regression
            prior.failure_count += 1
            prior.consecutive_failures += 1
            prior.consecutive_successes = 0

            if prior.consecutive_failures >= 3:
                prior.state = "suspicious"
            elif prior.consecutive_failures >= 2:
                prior.state = "mixed"

            prior.confidence = max(0.2, prior.confidence - 0.1)

        # Persist
        self.save_prior(eco_name, prior)
```

---

## Demo Output: Before/After Comparison Report

### Required Format for summary_comparison.txt

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                    NOODLE 2 - STUDY COMPLETION REPORT                        ║
║                         nangate45_extreme_demo                                ║
╚══════════════════════════════════════════════════════════════════════════════╝

EXECUTION SUMMARY
─────────────────────────────────────────────────────────────────────────────────
Study Name:           nangate45_extreme_demo
PDK:                  Nangate45 (45nm)
Design:               GCD
Safety Domain:        guarded
Start Time:           2026-01-12 21:30:00
End Time:             2026-01-12 21:45:32
Total Runtime:        15 minutes 32 seconds

BEFORE/AFTER COMPARISON
─────────────────────────────────────────────────────────────────────────────────
Metric                    Before          After           Delta       Status
─────────────────────────────────────────────────────────────────────────────────
WNS (ps)                  -2850           -420            +2430       ✓ 85.3% improved
TNS (ps)                  -45000          -3200           +41800      ✓ 92.9% improved
Hot Ratio                 0.35            0.08            -0.27       ✓ 77.1% improved
Failing Endpoints         234             12              -222        ✓ 94.9% fixed
Area (um²)                12500           13200           +700        ⚠ 5.6% increase
Cell Count                4521            4892            +371        (buffers added)

STAGE PROGRESSION
─────────────────────────────────────────────────────────────────────────────────
Stage 0: Exploration
  - Trials executed:    15
  - Trials successful:  12
  - Trials failed:      2
  - Trials doomed:      1 (early termination saved 8 trials)
  - Best WNS achieved:  -1850 ps (35.1% improvement)
  - Survivors:          4

Stage 1: Refinement
  - Trials executed:    10
  - Trials successful:  9
  - Trials failed:      1
  - Best WNS achieved:  -720 ps (74.7% improvement from start)
  - Survivors:          3

Stage 2: Closure
  - Trials executed:    8
  - Trials successful:  8
  - Best WNS achieved:  -420 ps (85.3% improvement from start)
  - Final survivor:     1

ECO EFFECTIVENESS
─────────────────────────────────────────────────────────────────────────────────
ECO Name                  Applied    Success    Avg WNS Δ    Prior State
─────────────────────────────────────────────────────────────────────────────────
cell_resize               18         15         +312 ps      trusted
buffer_insertion          12         10         +245 ps      trusted
gate_cloning              5          4          +156 ps      trusted
placement_refinement      3          2          +89 ps       mixed

RAIL COMPLIANCE
─────────────────────────────────────────────────────────────────────────────────
Rail Type            Threshold       Worst Case      Status
─────────────────────────────────────────────────────────────────────────────────
abort_wns_ps         -10000          -2850           ✓ Never triggered
abort_timeout_s      600             45              ✓ Never triggered
stage_failure_rate   0.80            0.20            ✓ Never triggered
study_catastrophic   5               0               ✓ Never triggered

DOOM DETECTION
─────────────────────────────────────────────────────────────────────────────────
Cases Terminated Early:   1
Compute Saved:            8 trials (~5 minutes)
Doom Type:                trajectory_doom
Reason:                   3 consecutive stages with <2% improvement

ARTIFACTS GENERATED
─────────────────────────────────────────────────────────────────────────────────
[✓] study_summary.json
[✓] safety_trace.json (33 trials)
[✓] priors_export.json
[✓] lineage.dot (case DAG)
[✓] heatmaps/placement_density_before.png
[✓] heatmaps/placement_density_after.png
[✓] heatmaps/placement_density_diff.png
[✓] heatmaps/routing_congestion_before.png
[✓] heatmaps/routing_congestion_after.png
[✓] heatmaps/routing_congestion_diff.png
[✓] charts/wns_trajectory.png
[✓] charts/hot_ratio_trajectory.png
[✓] charts/pareto_frontier.png
[✓] charts/pareto_evolution.gif
[✓] charts/eco_effectiveness.png

CONCLUSION
─────────────────────────────────────────────────────────────────────────────────
Status: SUCCESS - All objectives met

The design was successfully optimized from an extreme broken state to a
production-viable state. WNS improved by 85.3%, exceeding the 50% target.
Hot ratio reduced by 77.1%, well below the 0.12 threshold.

All safety rails were respected. 1 doomed case was terminated early,
saving compute resources. Priors were updated and exported for future studies.

╔══════════════════════════════════════════════════════════════════════════════╗
║  This report was generated by ACTUAL EXECUTION, not simulation or mocking.   ║
║  All metrics are from real OpenROAD runs. All visualizations are real PNGs.  ║
╚══════════════════════════════════════════════════════════════════════════════╝
```

---

## Failure Injection for Demo (Controlled Doom)

### Ensuring the Demo Shows Early Termination

To demonstrate doom detection, we intentionally create a case that will fail:

```python
class DemoFailureInjector:
    """Injects controlled failures for demo purposes."""

    def inject_doomed_trial(self, stage: int, trial_index: int) -> ECOConfig:
        """
        Create a trial configuration that will reliably produce doom.

        Strategy: Apply an ECO with parameters known to fail on this design.
        For example, aggressive gate cloning on a design with no high-fanout nets.
        """
        if trial_index == 3 and stage == 0:  # Inject doom at trial 3 of stage 0
            return ECOConfig(
                name="gate_cloning",
                params={
                    "max_fanout": 4,  # Too aggressive - will find nothing to clone
                },
                _is_doom_injection=True  # Internal flag for audit
            )
        return None  # Normal trial
```

This ensures the demo always shows at least one doomed case being terminated early.

---

## Console Output During Demo

### Expected Output Format

```
╔══════════════════════════════════════════════════════════════════════════════╗
║              NOODLE 2 - NANGATE45 EXTREME DEMO                               ║
╚══════════════════════════════════════════════════════════════════════════════╝

Initializing Ray with dashboard...
════════════════════════════════════════════════════════════════════════════════
 RAY DASHBOARD: http://localhost:8265
 Open this URL to monitor trial execution in real-time
════════════════════════════════════════════════════════════════════════════════

Loading study configuration...
  Study: nangate45_extreme_demo
  PDK: Nangate45
  Safety domain: guarded
  Stages: 3

=== BASE CASE VERIFICATION ===
Loading extreme snapshot: studies/nangate45_extreme/gcd_extreme.odb
Running initial STA...
  WNS: -2850 ps (TIMING VIOLATION - this is expected)
  TNS: -45000 ps
  Hot ratio: 0.35
✓ Base case verified - design has timing violations to fix

=== STAGE 0: EXPLORATION ===
Trial budget: 15 | Survivors: 4 | ECO classes: [placement_local]

  [Trial 1/15] cell_resize (max_wire_length=100)
    WNS: -2850 → -2120 ps (+730 ps improvement)
    Status: ✓ SUCCESS

  [Trial 2/15] buffer_insertion (slack_margin=50)
    WNS: -2850 → -2340 ps (+510 ps improvement)
    Status: ✓ SUCCESS

  [Trial 3/15] gate_cloning (max_fanout=4)
    WNS: -2850 → -2820 ps (+30 ps improvement)
    Trajectory analysis: <2% improvement for 3 consecutive attempts
    Status: ⚠ DOOMED - Terminating early (saving 8 remaining trials)

  [Trial 4/15] cell_resize (max_wire_length=80)
    WNS: -2850 → -1950 ps (+900 ps improvement)
    Status: ✓ SUCCESS

  ... (12 more trials)

Stage 0 Complete:
  Survivors: [trial_4, trial_7, trial_11, trial_14]
  Best WNS: -1850 ps (35.1% improvement)
  Doomed cases: 1 (compute saved: ~5 min)
  Checkpoint saved: checkpoints/stage_0_complete.noodle

=== STAGE 1: REFINEMENT ===
... (similar output)

=== STAGE 2: CLOSURE ===
... (similar output)

=== STUDY COMPLETE ===

FINAL RESULTS:
─────────────────────────────────────────────────────────────────────────────────
  WNS:       -2850 ps → -420 ps   (85.3% IMPROVEMENT) ✓
  Hot ratio: 0.35 → 0.08          (77.1% REDUCTION) ✓

  Total trials: 33
  Successful: 29
  Failed: 3
  Doomed (early terminated): 1
  Runtime: 15 min 32 sec

Artifacts saved to: demo_output/nangate45_extreme_demo/
  - summary_comparison.txt
  - safety_trace.json
  - heatmaps/ (6 PNG files)
  - charts/ (5 PNG files)

Ray shutdown complete.

════════════════════════════════════════════════════════════════════════════════
 DEMO COMPLETE - All objectives achieved with ACTUAL execution (no mocking)
════════════════════════════════════════════════════════════════════════════════
```

---

## Post-Demo Verification Script

### Script to Verify Demo Output is Real

```bash
#!/bin/bash
# verify_demo_output.sh - Verify demo produced real artifacts

DEMO_DIR="$1"

echo "=== Verifying Demo Output ==="

# Check PNGs are real images
echo "Checking PNG files..."
for png in $(find "$DEMO_DIR" -name "*.png"); do
    file_type=$(file "$png" | grep -c "PNG image")
    if [ "$file_type" -eq 0 ]; then
        echo "FAIL: $png is NOT a real PNG image"
        exit 1
    fi
    # Check image has actual dimensions
    dimensions=$(identify -format "%wx%h" "$png" 2>/dev/null)
    if [ -z "$dimensions" ]; then
        echo "FAIL: $png has no dimensions (placeholder?)"
        exit 1
    fi
    echo "  ✓ $png ($dimensions)"
done

# Check metrics.json has real values
echo "Checking metrics files..."
for metrics in $(find "$DEMO_DIR" -name "metrics.json"); do
    wns=$(jq '.wns_ps' "$metrics")
    if [ "$wns" == "null" ]; then
        echo "FAIL: $metrics has no wns_ps"
        exit 1
    fi
    echo "  ✓ $metrics (WNS: $wns)"
done

# Check summary has mocking=false
echo "Checking summary.json..."
summary="$DEMO_DIR/summary.json"
mocking=$(jq '.mocking' "$summary")
if [ "$mocking" != "false" ]; then
    echo "FAIL: summary.json shows mocking=$mocking"
    exit 1
fi
echo "  ✓ mocking: false"

# Check safety traces exist
trace_count=$(find "$DEMO_DIR" -name "safety_trace.json" | wc -l)
echo "  ✓ Safety traces: $trace_count"

echo ""
echo "=== ALL VERIFICATIONS PASSED ==="
echo "Demo output is REAL, not mocked or placeholder"
```

---

## Docker Execution Contract

### Exact Docker Command for Trial Execution

Each trial MUST be executed using this exact Docker invocation:

```bash
#!/bin/bash
# run_trial.sh - Execute a single trial in Docker

ORFS_IMAGE="openroad/orfs:latest"
OPENROAD_BIN="/OpenROAD-flow-scripts/tools/install/OpenROAD/bin/openroad"
WORK_DIR="$1"        # Host directory for trial artifacts
TCL_SCRIPT="$2"      # TCL script to execute
TIMEOUT_SEC="${3:-600}"

docker run --rm \
    -v "$WORK_DIR:/work" \
    -v "/tmp/.X11-unix:/tmp/.X11-unix" \
    -e DISPLAY=:99 \
    -w /work \
    --memory=8g \
    --cpus=2 \
    "$ORFS_IMAGE" \
    timeout "$TIMEOUT_SEC" \
    "$OPENROAD_BIN" -exit "$TCL_SCRIPT"

exit_code=$?

if [ $exit_code -eq 124 ]; then
    echo "TIMEOUT: Trial exceeded ${TIMEOUT_SEC}s"
    exit 124
fi

exit $exit_code
```

### PDK Paths Inside ORFS Container

**CRITICAL**: These are the EXACT paths inside the `openroad/orfs:latest` container:

```bash
# Base paths
ORFS_ROOT="/OpenROAD-flow-scripts"
PLATFORMS="$ORFS_ROOT/flow/platforms"
DESIGNS="$ORFS_ROOT/flow/designs"

# Nangate45 (PREFERRED for demos)
NANGATE45_DIR="$PLATFORMS/nangate45"
NANGATE45_TECH_LEF="$NANGATE45_DIR/lef/NangateOpenCellLibrary.tech.lef"
NANGATE45_MACRO_LEF="$NANGATE45_DIR/lef/NangateOpenCellLibrary.macro.mod.lef"
NANGATE45_LIB="$NANGATE45_DIR/lib/NangateOpenCellLibrary_typical.lib"
NANGATE45_GCD_VERILOG="$DESIGNS/nangate45/gcd/gcd.v"

# ASAP7
ASAP7_DIR="$PLATFORMS/asap7"
ASAP7_TECH_LEF="$ASAP7_DIR/lef/asap7_tech_1x_201209.lef"
ASAP7_CELLS_LEF="$ASAP7_DIR/lef/asap7sc7p5t_28_L_4x_220121a.lef"
ASAP7_LIB="$ASAP7_DIR/lib/asap7sc7p5t_SEQ_RVT_TT_nldm_220123.lib"
ASAP7_GCD_VERILOG="$DESIGNS/asap7/gcd/gcd.v"

# Sky130
SKY130_DIR="$PLATFORMS/sky130hd"
SKY130_TECH_LEF="$SKY130_DIR/lef/sky130_fd_sc_hd.tlef"
SKY130_CELLS_LEF="$SKY130_DIR/lef/sky130_fd_sc_hd_merged.lef"
SKY130_LIB="$SKY130_DIR/lib/sky130_fd_sc_hd__tt_025C_1v80.lib"
SKY130_GCD_VERILOG="$DESIGNS/sky130hd/gcd/gcd.v"
```

### Complete Trial TCL Template

```tcl
# trial_template.tcl - Complete trial execution template
# This template MUST be used for all trials

# ========================================
# SECTION 1: ENVIRONMENT SETUP
# ========================================

# PDK selection (set by trial config)
set pdk "nangate45"  ;# or "asap7", "sky130hd"

# Set paths based on PDK
switch $pdk {
    "nangate45" {
        set tech_lef "/OpenROAD-flow-scripts/flow/platforms/nangate45/lef/NangateOpenCellLibrary.tech.lef"
        set macro_lef "/OpenROAD-flow-scripts/flow/platforms/nangate45/lef/NangateOpenCellLibrary.macro.mod.lef"
        set liberty "/OpenROAD-flow-scripts/flow/platforms/nangate45/lib/NangateOpenCellLibrary_typical.lib"
        set buffer_cell "BUF_X4"
    }
    "asap7" {
        set tech_lef "/OpenROAD-flow-scripts/flow/platforms/asap7/lef/asap7_tech_1x_201209.lef"
        set macro_lef "/OpenROAD-flow-scripts/flow/platforms/asap7/lef/asap7sc7p5t_28_L_4x_220121a.lef"
        set liberty "/OpenROAD-flow-scripts/flow/platforms/asap7/lib/asap7sc7p5t_SEQ_RVT_TT_nldm_220123.lib"
        set buffer_cell "BUFx4_ASAP7_75t_R"
    }
    "sky130hd" {
        set tech_lef "/OpenROAD-flow-scripts/flow/platforms/sky130hd/lef/sky130_fd_sc_hd.tlef"
        set macro_lef "/OpenROAD-flow-scripts/flow/platforms/sky130hd/lef/sky130_fd_sc_hd_merged.lef"
        set liberty "/OpenROAD-flow-scripts/flow/platforms/sky130hd/lib/sky130_fd_sc_hd__tt_025C_1v80.lib"
        set buffer_cell "sky130_fd_sc_hd__buf_4"
    }
}

# ========================================
# SECTION 2: LOAD DESIGN
# ========================================

read_lef $tech_lef
read_lef $macro_lef
read_liberty $liberty

# Load snapshot (ODB file from previous stage or base case)
set snapshot_path [lindex $argv 0]
read_db $snapshot_path

# ========================================
# SECTION 3: PRE-ECO METRICS
# ========================================

# Capture metrics BEFORE applying ECO
set_wire_rc -signal -layer metal3
estimate_parasitics -placement

# Get timing metrics
set pre_wns [sta::worst_slack -max]
set pre_tns [sta::total_negative_slack -max]

puts "PRE_ECO_WNS_NS: $pre_wns"
puts "PRE_ECO_TNS_NS: $pre_tns"

# ========================================
# SECTION 4: APPLY ECO
# ========================================

# ECO parameters (injected by trial configuration)
set eco_name [lindex $argv 1]
set eco_params_json [lindex $argv 2]

# Parse ECO parameters (simplified - real implementation uses proper JSON parsing)
# Example: {"max_wire_length": 100, "buffer_cell": "BUF_X4"}

switch $eco_name {
    "cell_resize" {
        # Extract parameters from JSON
        regexp {max_wire_length.*?(\d+)} $eco_params_json -> max_wire_length
        if {![info exists max_wire_length]} { set max_wire_length 100 }

        # Apply repair_design ECO
        repair_design \
            -max_wire_length $max_wire_length \
            -buffer_cell $buffer_cell \
            -max_utilization 0.85
    }
    "buffer_insertion" {
        regexp {slack_margin.*?(\d+)} $eco_params_json -> slack_margin
        regexp {max_buffer_percent.*?(\d+)} $eco_params_json -> max_buffer_percent
        if {![info exists slack_margin]} { set slack_margin 50 }
        if {![info exists max_buffer_percent]} { set max_buffer_percent 20 }

        repair_timing \
            -setup \
            -slack_margin $slack_margin \
            -max_buffer_percent $max_buffer_percent
    }
    "gate_cloning" {
        regexp {max_fanout.*?(\d+)} $eco_params_json -> max_fanout
        if {![info exists max_fanout]} { set max_fanout 16 }

        repair_timing \
            -setup \
            -max_fanout $max_fanout
    }
    "placement_refinement" {
        detailed_placement
        optimize_mirroring
    }
    "hold_fix" {
        regexp {hold_slack_margin.*?(\d+)} $eco_params_json -> hold_slack_margin
        if {![info exists hold_slack_margin]} { set hold_slack_margin 50 }

        repair_timing \
            -hold \
            -slack_margin $hold_slack_margin
    }
    default {
        puts "WARNING: Unknown ECO '$eco_name', running no-op"
    }
}

# ========================================
# SECTION 5: POST-ECO METRICS
# ========================================

# Re-estimate parasitics after changes
estimate_parasitics -placement

# Get new timing metrics
set post_wns [sta::worst_slack -max]
set post_tns [sta::total_negative_slack -max]

puts "POST_ECO_WNS_NS: $post_wns"
puts "POST_ECO_TNS_NS: $post_tns"

# Calculate improvement
set wns_delta [expr {$post_wns - $pre_wns}]
set wns_improvement_pct [expr {100.0 * $wns_delta / abs($pre_wns)}]

puts "WNS_DELTA_NS: $wns_delta"
puts "WNS_IMPROVEMENT_PCT: $wns_improvement_pct"

# ========================================
# SECTION 6: DETAILED TIMING REPORT
# ========================================

# Report critical paths for diagnosis
report_checks -path_delay max -fields {slew cap input_pin net fanout} -digits 4 \
    -format full_clock_expanded > timing_report.txt

# Report worst slack
report_worst_slack -max >> timing_report.txt
report_tns >> timing_report.txt

# ========================================
# SECTION 7: SAVE MODIFIED SNAPSHOT
# ========================================

set output_db [lindex $argv 3]
write_db $output_db

puts "TRIAL_COMPLETE"
puts "OUTPUT_DB: $output_db"

exit 0
```

---

## Differential Heatmap Algorithm

### Computing Before/After Differences

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import TwoSlopeNorm

def compute_differential_heatmap(
    before_csv: Path,
    after_csv: Path,
    output_png: Path,
    title: str = "Differential Heatmap"
) -> dict:
    """
    Compute and visualize differential heatmap.

    Returns metrics about the difference.
    """
    # Load heatmap data from OpenROAD CSV exports
    before_data = np.loadtxt(before_csv, delimiter=',')
    after_data = np.loadtxt(after_csv, delimiter=',')

    # Ensure same dimensions
    assert before_data.shape == after_data.shape, \
        f"Shape mismatch: {before_data.shape} vs {after_data.shape}"

    # Compute difference: negative = improved, positive = worsened
    # For density/congestion metrics, lower is better
    diff_data = after_data - before_data

    # Compute statistics
    improved_bins = np.sum(diff_data < -0.01)  # >1% improvement
    worsened_bins = np.sum(diff_data > 0.01)   # >1% worsening
    unchanged_bins = np.sum(np.abs(diff_data) <= 0.01)

    total_bins = diff_data.size
    net_improvement_pct = 100.0 * (improved_bins - worsened_bins) / total_bins

    # Find region with maximum improvement
    min_idx = np.unravel_index(np.argmin(diff_data), diff_data.shape)
    max_improvement_region = {
        "x": int(min_idx[1]),
        "y": int(min_idx[0]),
        "delta": float(diff_data[min_idx])
    }

    # Create visualization
    fig, ax = plt.subplots(figsize=(12, 10))

    # Use diverging colormap: green = improved, red = worsened
    vmax = max(abs(np.min(diff_data)), abs(np.max(diff_data)))
    norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)

    im = ax.imshow(
        diff_data,
        cmap='RdYlGn_r',  # Red=worse, Green=better
        norm=norm,
        interpolation='nearest',
        origin='lower'
    )

    # Add colorbar
    cbar = plt.colorbar(im, ax=ax, shrink=0.8)
    cbar.set_label('Change (negative = improved)', fontsize=12)

    # Add title and labels
    ax.set_title(f'{title}\nImproved: {improved_bins} | Worsened: {worsened_bins} | Net: {net_improvement_pct:.1f}%', fontsize=14)
    ax.set_xlabel('X Bins')
    ax.set_ylabel('Y Bins')

    # Mark max improvement region
    ax.plot(max_improvement_region['x'], max_improvement_region['y'],
            'b*', markersize=15, markeredgecolor='white', markeredgewidth=2)
    ax.annotate(f"Max improvement: {max_improvement_region['delta']:.3f}",
                xy=(max_improvement_region['x'], max_improvement_region['y']),
                xytext=(10, 10), textcoords='offset points',
                fontsize=10, color='blue',
                arrowprops=dict(arrowstyle='->', color='blue'))

    # Save as REAL PNG (not placeholder)
    plt.savefig(output_png, dpi=150, bbox_inches='tight')
    plt.close()

    return {
        "improved_bins": int(improved_bins),
        "worsened_bins": int(worsened_bins),
        "unchanged_bins": int(unchanged_bins),
        "net_improvement_pct": float(net_improvement_pct),
        "max_improvement_region": max_improvement_region
    }
```

---

## Pareto Frontier Algorithm

### Identifying Non-Dominated Solutions

```python
from typing import List, Tuple
import numpy as np

def identify_pareto_frontier(
    solutions: List[dict],
    objectives: List[str] = ["wns_ps", "hot_ratio"],
    minimize: List[bool] = [True, True]  # True = minimize (more negative WNS is worse)
) -> Tuple[List[dict], List[dict]]:
    """
    Identify Pareto-optimal solutions.

    Args:
        solutions: List of trial results with metrics
        objectives: Which metrics to consider
        minimize: Whether each objective should be minimized

    Returns:
        (pareto_solutions, dominated_solutions)

    For WNS: Higher (less negative) is better, so we want to MAXIMIZE
    For hot_ratio: Lower is better, so we want to MINIMIZE

    Example:
        solutions = [
            {"trial_id": "t1", "wns_ps": -1000, "hot_ratio": 0.2},
            {"trial_id": "t2", "wns_ps": -800, "hot_ratio": 0.25},
            {"trial_id": "t3", "wns_ps": -1200, "hot_ratio": 0.15},
        ]

        t2 dominates t1 if t2 is better in at least one objective
        and not worse in any objective.
    """
    n = len(solutions)

    # Extract objective values
    # Convert WNS to "higher is better" by negating
    obj_values = np.zeros((n, len(objectives)))
    for i, sol in enumerate(solutions):
        for j, (obj, mini) in enumerate(zip(objectives, minimize)):
            val = sol.get(obj, float('inf'))
            # Negate if we're minimizing (so we can use "higher is better" throughout)
            obj_values[i, j] = val if not mini else -val

    # Find dominated solutions
    is_dominated = np.zeros(n, dtype=bool)

    for i in range(n):
        for j in range(n):
            if i == j:
                continue

            # Check if j dominates i
            # j dominates i if j is >= in all objectives and > in at least one
            at_least_as_good = np.all(obj_values[j] >= obj_values[i])
            strictly_better = np.any(obj_values[j] > obj_values[i])

            if at_least_as_good and strictly_better:
                is_dominated[i] = True
                break

    pareto = [solutions[i] for i in range(n) if not is_dominated[i]]
    dominated = [solutions[i] for i in range(n) if is_dominated[i]]

    return pareto, dominated


def visualize_pareto_frontier(
    solutions: List[dict],
    pareto: List[dict],
    output_png: Path,
    x_obj: str = "wns_ps",
    y_obj: str = "hot_ratio"
) -> None:
    """Generate Pareto frontier visualization."""
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(10, 8))

    # Plot dominated solutions
    dominated = [s for s in solutions if s not in pareto]
    dom_x = [s[x_obj] for s in dominated]
    dom_y = [s[y_obj] for s in dominated]
    ax.scatter(dom_x, dom_y, c='gray', marker='x', s=100, label='Dominated', alpha=0.6)

    # Plot Pareto-optimal solutions
    pareto_x = [s[x_obj] for s in pareto]
    pareto_y = [s[y_obj] for s in pareto]
    ax.scatter(pareto_x, pareto_y, c='green', marker='o', s=150, label='Pareto-optimal',
               edgecolors='darkgreen', linewidths=2)

    # Draw Pareto frontier line
    if len(pareto) > 1:
        # Sort by x for line drawing
        pareto_sorted = sorted(pareto, key=lambda s: s[x_obj])
        frontier_x = [s[x_obj] for s in pareto_sorted]
        frontier_y = [s[y_obj] for s in pareto_sorted]
        ax.plot(frontier_x, frontier_y, 'g--', linewidth=2, alpha=0.7, label='Frontier')

    # Labels
    ax.set_xlabel('WNS (ps) - higher is better', fontsize=12)
    ax.set_ylabel('Hot Ratio - lower is better', fontsize=12)
    ax.set_title('Pareto Frontier: WNS vs Congestion Tradeoff', fontsize=14)
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3)

    # Invert y-axis since lower hot_ratio is better
    ax.invert_yaxis()

    plt.savefig(output_png, dpi=150, bbox_inches='tight')
    plt.close()
```

---

## Ray Worker Isolation

### Ensuring Clean State for Each Trial

```python
import ray
import tempfile
import shutil
from pathlib import Path

@ray.remote
class IsolatedTrialWorker:
    """
    Ray actor that executes trials with complete isolation.

    Key isolation guarantees:
    1. Each trial gets a fresh working directory
    2. Snapshot is copied (not symlinked) to prevent corruption
    3. All artifacts are written to trial-specific paths
    4. Cleanup happens even on failure
    """

    def __init__(self, worker_id: int, base_artifacts_dir: Path):
        self.worker_id = worker_id
        self.base_artifacts_dir = Path(base_artifacts_dir)
        self.work_dir = None

    def execute_trial(
        self,
        trial_id: str,
        snapshot_path: Path,
        eco_config: dict,
        timeout_seconds: int = 600
    ) -> dict:
        """Execute a single trial with full isolation."""

        # Create isolated working directory
        self.work_dir = Path(tempfile.mkdtemp(prefix=f"noodle2_trial_{trial_id}_"))

        try:
            # Copy snapshot to isolated directory (not symlink!)
            isolated_snapshot = self.work_dir / "snapshot.odb"
            shutil.copy2(snapshot_path, isolated_snapshot)

            # Generate TCL script
            tcl_script = self.work_dir / "trial.tcl"
            self._generate_trial_tcl(tcl_script, isolated_snapshot, eco_config)

            # Execute in Docker
            result = self._run_docker_trial(
                tcl_script,
                timeout_seconds
            )

            # Collect artifacts
            artifacts = self._collect_artifacts(trial_id)

            return {
                "trial_id": trial_id,
                "worker_id": self.worker_id,
                "success": result["exit_code"] == 0,
                "exit_code": result["exit_code"],
                "metrics": result.get("metrics", {}),
                "artifacts": artifacts,
                "work_dir": str(self.work_dir)
            }

        except Exception as e:
            return {
                "trial_id": trial_id,
                "worker_id": self.worker_id,
                "success": False,
                "exit_code": -1,
                "error": str(e),
                "work_dir": str(self.work_dir)
            }
        finally:
            # Archive artifacts to permanent storage
            self._archive_artifacts(trial_id)

    def _archive_artifacts(self, trial_id: str) -> None:
        """Copy artifacts from temp dir to permanent storage."""
        if self.work_dir and self.work_dir.exists():
            trial_artifacts_dir = self.base_artifacts_dir / trial_id
            trial_artifacts_dir.mkdir(parents=True, exist_ok=True)

            # Copy key artifacts
            for artifact in ["metrics.json", "timing_report.txt", "safety_trace.json"]:
                src = self.work_dir / artifact
                if src.exists():
                    shutil.copy2(src, trial_artifacts_dir / artifact)

            # Copy heatmaps
            heatmap_src = self.work_dir / "heatmaps"
            if heatmap_src.exists():
                shutil.copytree(heatmap_src, trial_artifacts_dir / "heatmaps", dirs_exist_ok=True)

    def cleanup(self) -> None:
        """Clean up temporary files."""
        if self.work_dir and self.work_dir.exists():
            shutil.rmtree(self.work_dir, ignore_errors=True)
```

---

## Error Recovery and Cleanup

### Handling Partial Failures

```python
class TrialErrorRecovery:
    """
    Handles recovery from partial trial failures.

    Error types and recovery strategies:
    1. Docker crash: Log error, mark trial failed, continue
    2. Timeout: Kill container, mark trial timed_out, continue
    3. Parse error: Mark metrics as unavailable, log warning
    4. Disk full: Emergency cleanup, abort stage
    5. Network error: Retry with exponential backoff
    """

    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries
        self.retry_delays = [1, 5, 15]  # seconds

    def handle_docker_crash(self, trial_id: str, error: Exception) -> dict:
        """Handle Docker container crash."""
        return {
            "trial_id": trial_id,
            "failure_type": "tool_crash",
            "severity": "critical",
            "recoverable": False,
            "error_message": str(error),
            "action": "mark_failed_continue"
        }

    def handle_timeout(self, trial_id: str, timeout_sec: int) -> dict:
        """Handle trial timeout."""
        # Kill any running container
        self._kill_trial_container(trial_id)

        return {
            "trial_id": trial_id,
            "failure_type": "timeout",
            "severity": "high",
            "recoverable": False,
            "timeout_seconds": timeout_sec,
            "action": "mark_timeout_continue"
        }

    def handle_parse_error(self, trial_id: str, file_path: Path, error: Exception) -> dict:
        """Handle metric parsing errors."""
        return {
            "trial_id": trial_id,
            "failure_type": "parse_failure",
            "severity": "medium",
            "recoverable": True,
            "file_path": str(file_path),
            "error_message": str(error),
            "action": "use_fallback_metrics"
        }

    def handle_disk_full(self, trial_id: str) -> dict:
        """Handle disk space exhaustion - CRITICAL."""
        # Emergency cleanup
        self._emergency_cleanup()

        return {
            "trial_id": trial_id,
            "failure_type": "disk_full",
            "severity": "critical",
            "recoverable": False,
            "action": "abort_stage"
        }

    def _kill_trial_container(self, trial_id: str) -> None:
        """Force kill the Docker container for a trial."""
        import subprocess
        container_name = f"noodle2_trial_{trial_id}"
        subprocess.run(
            ["docker", "kill", container_name],
            capture_output=True,
            timeout=10
        )

    def _emergency_cleanup(self) -> None:
        """Emergency cleanup to free disk space."""
        import subprocess

        # Remove Docker build cache
        subprocess.run(["docker", "system", "prune", "-f"], capture_output=True)

        # Clean old artifacts (keep only last 10 studies)
        # Implementation depends on artifact storage location
```

---

## Checkpoint Format and Verification

### Checkpoint File Structure

```python
import json
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Optional

@dataclass
class Checkpoint:
    """
    Checkpoint format for study resumption.

    Checkpoints are saved at stage boundaries and can be used
    to resume a study after interruption.
    """
    version: str = "1.0"
    study_name: str = ""
    study_hash: str = ""  # Hash of study configuration

    # Progress tracking
    completed_stages: int = 0
    current_stage: int = 0
    trials_in_stage: int = 0

    # State
    survivors: List[str] = None  # List of surviving case IDs
    priors_snapshot: dict = None  # ECO priors at checkpoint

    # Verification
    snapshot_hashes: dict = None  # {case_id: snapshot_hash}
    checkpoint_hash: str = ""     # Hash of checkpoint contents

    def save(self, path: Path) -> None:
        """Save checkpoint with integrity verification."""
        # Compute hash before saving (excluding checkpoint_hash field)
        data = asdict(self)
        data['checkpoint_hash'] = ''  # Clear for hashing
        content = json.dumps(data, sort_keys=True)
        data['checkpoint_hash'] = hashlib.sha256(content.encode()).hexdigest()

        # Save atomically (write to temp, then rename)
        temp_path = path.with_suffix('.tmp')
        with open(temp_path, 'w') as f:
            json.dump(data, f, indent=2)
        temp_path.rename(path)

    @classmethod
    def load(cls, path: Path) -> 'Checkpoint':
        """Load and verify checkpoint."""
        with open(path) as f:
            data = json.load(f)

        # Verify integrity
        stored_hash = data.get('checkpoint_hash', '')
        data['checkpoint_hash'] = ''
        content = json.dumps(data, sort_keys=True)
        computed_hash = hashlib.sha256(content.encode()).hexdigest()

        if stored_hash != computed_hash:
            raise ValueError(f"Checkpoint integrity check failed: {path}")

        data['checkpoint_hash'] = stored_hash
        return cls(**data)

    def verify_snapshots(self, snapshot_dir: Path) -> bool:
        """Verify all snapshot files are intact."""
        for case_id, expected_hash in self.snapshot_hashes.items():
            snapshot_path = snapshot_dir / f"{case_id}.odb"
            if not snapshot_path.exists():
                return False

            actual_hash = self._hash_file(snapshot_path)
            if actual_hash != expected_hash:
                return False

        return True

    @staticmethod
    def _hash_file(path: Path) -> str:
        """Compute SHA-256 hash of a file."""
        h = hashlib.sha256()
        with open(path, 'rb') as f:
            while chunk := f.read(8192):
                h.update(chunk)
        return h.hexdigest()
```

---

## PDK-Specific Anti-Patterns

### Known Failure Patterns by PDK

```yaml
# anti_patterns_by_pdk.yaml
# These patterns have been observed to consistently fail

nangate45:
  anti_patterns:
    - name: aggressive_buffering_high_utilization
      pattern: "buffer_insertion when utilization > 0.8"
      failure_rate: 0.65
      outcome: "placement density violations"
      recommendation: "reduce max_buffer_percent to 10% or lower utilization first"

    - name: resize_on_congested_design
      pattern: "cell_resize when hot_ratio > 0.4"
      failure_rate: 0.58
      outcome: "timing improves but congestion explodes"
      recommendation: "apply placement_refinement first"

asap7:
  anti_patterns:
    - name: aggressive_buffering
      pattern: "buffer_insertion with max_buffer_percent > 15"
      failure_rate: 0.78
      outcome: "placement density violations, legalization failure"
      recommendation: "limit max_buffer_percent to 10% for ASAP7"

    - name: default_wire_length
      pattern: "cell_resize with default max_wire_length (100)"
      failure_rate: 0.52
      outcome: "excessive buffer insertion for 7nm scale"
      recommendation: "use max_wire_length 30-50 for ASAP7"

    - name: early_hold_fix
      pattern: "hold_fix in stage 0 or 1"
      failure_rate: 0.71
      outcome: "hold buffers conflict with setup optimization"
      recommendation: "defer hold_fix to final closure stage"

sky130hd:
  anti_patterns:
    - name: small_buffer_cell
      pattern: "using buf_1 or buf_2 buffer cells"
      failure_rate: 0.45
      outcome: "insufficient drive strength for long wires"
      recommendation: "use buf_4 or buf_8 for Sky130"

    - name: aggressive_clock_constraint
      pattern: "clock period < 5ns"
      failure_rate: 0.82
      outcome: "unrealistic for 130nm, no achievable solution"
      recommendation: "use clock period >= 8ns for realistic demos"
```

---

## Module Communication Contracts

### Inter-Module Data Flow

```python
# Communication contracts between Noodle 2 modules

# 1. Study Controller -> Trial Executor
@dataclass
class TrialRequest:
    """Request from controller to executor."""
    trial_id: str
    snapshot_path: Path
    eco_config: ECOConfig
    timeout_seconds: int
    output_dir: Path

    # Required outputs
    required_artifacts: List[str] = field(default_factory=lambda: [
        "metrics.json",
        "timing_report.txt",
        "safety_trace.json"
    ])

# 2. Trial Executor -> Study Controller
@dataclass
class TrialResult:
    """Result from executor to controller."""
    trial_id: str
    success: bool
    exit_code: int

    # Metrics (all in consistent units)
    wns_ps: float          # Picoseconds (negative = violation)
    tns_ps: float          # Picoseconds
    hot_ratio: float       # 0.0 to 1.0
    area_um2: float        # Square microns

    # Delta from parent
    wns_delta_ps: float
    tns_delta_ps: float

    # Classification
    failure_type: Optional[str]  # None if success
    failure_severity: Optional[str]

    # Artifacts
    artifacts_dir: Path
    output_snapshot: Path  # Modified snapshot for chaining

    # Timing
    execution_time_seconds: float

# 3. Auto-Diagnosis -> ECO Selector
@dataclass
class DiagnosisResult:
    """Diagnosis output for ECO selection."""
    primary_issue: str      # "timing" or "congestion"
    secondary_issue: Optional[str]
    confidence: float       # 0.0 to 1.0

    # Timing details
    timing: Optional[TimingDiagnosis]

    # Congestion details
    congestion: Optional[CongestionDiagnosis]

    # Recommended ECOs in priority order
    recommended_ecos: List[str]

    # Anti-patterns to avoid
    avoid_patterns: List[str]

# 4. ECO Selector -> Trial Generator
@dataclass
class ECOConfig:
    """ECO configuration for trial generation."""
    eco_name: str
    eco_class: str  # topology_neutral, placement_local, etc.
    parameters: dict

    # Prior information
    prior_state: str        # unknown, trusted, mixed, suspicious
    prior_success_rate: float
    expected_wns_improvement_ps: float

    # Safety
    preconditions_met: bool
    safety_domain_permitted: bool

# 5. Visualization Module -> Artifact Bundle
@dataclass
class VisualizationOutput:
    """Output from visualization module."""
    heatmaps: dict[str, Path]      # {name: path to PNG}
    differential: dict[str, Path]  # {name: path to diff PNG}
    overlays: dict[str, Path]      # {name: path to overlay PNG}
    charts: dict[str, Path]        # {name: path to chart PNG}

    # Metadata
    all_real_images: bool  # False if any placeholder
    generation_time_seconds: float
```

---

## Final Production Checklist

### Pre-Launch Verification

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                    NOODLE 2 - PRODUCTION READINESS CHECKLIST                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

DOCKER & ENVIRONMENT
─────────────────────────────────────────────────────────────────────────────────
[ ] ORFS container pulls successfully: docker pull openroad/orfs:latest
[ ] OpenROAD binary exists: /OpenROAD-flow-scripts/tools/install/OpenROAD/bin/openroad
[ ] All three PDKs accessible inside container (nangate45, asap7, sky130hd)
[ ] Xvfb available for headless heatmap generation
[ ] Python 3.10+ with numpy, matplotlib installed in container

ECO EXECUTION
─────────────────────────────────────────────────────────────────────────────────
[ ] repair_design command works with buffer insertion
[ ] repair_timing -setup command fixes setup violations
[ ] repair_timing -hold command fixes hold violations
[ ] detailed_placement command runs without error
[ ] Each ECO produces DIFFERENT metrics (not identical across trials)
[ ] Snapshot modification verified (before.odb != after.odb)

METRICS & PARSING
─────────────────────────────────────────────────────────────────────────────────
[ ] WNS extracted correctly from report_checks output
[ ] TNS extracted correctly from report_tns output
[ ] Hot ratio computed from global_route -congestion_report_file
[ ] All metrics in consistent units (ps for timing, ratio for congestion)
[ ] Metric parsing handles edge cases (no violations, extremely bad)

VISUALIZATION
─────────────────────────────────────────────────────────────────────────────────
[ ] All PNG files are REAL images (file command shows "PNG image")
[ ] Heatmaps generated from real OpenROAD gui::dump_heatmap data
[ ] Differential heatmaps show actual before/after differences
[ ] Pareto frontier identifies correct non-dominated solutions
[ ] All charts render without matplotlib errors

SAFETY & RAILS
─────────────────────────────────────────────────────────────────────────────────
[ ] Abort rails terminate trials exceeding thresholds
[ ] Stage rails stop stages with excessive failures
[ ] Study rails halt studies on catastrophic failures
[ ] Immutable rules prevent modification of protected elements
[ ] Legality check blocks illegal ECO/domain combinations

DOOM DETECTION
─────────────────────────────────────────────────────────────────────────────────
[ ] Metric-hopeless detection works (WNS < -10000ps)
[ ] Trajectory doom detection works (3 stages of stagnation)
[ ] ECO exhaustion detection works (all ECOs failed)
[ ] Compute savings logged when doom detected

PRIORS & LEARNING
─────────────────────────────────────────────────────────────────────────────────
[ ] Priors update after each trial (success increments, failure decrements)
[ ] Suspicious ECOs deprioritized in guarded domain
[ ] Warm-start loads priors from previous studies
[ ] Prior export/import produces valid JSON

RAY INTEGRATION
─────────────────────────────────────────────────────────────────────────────────
[ ] Ray initializes with dashboard enabled
[ ] Dashboard accessible at http://localhost:8265
[ ] Trials visible in Jobs tab during execution
[ ] Parallel trials complete faster than sequential
[ ] Clean shutdown without orphan workers

DEMO EXECUTION
─────────────────────────────────────────────────────────────────────────────────
[ ] Nangate45 demo achieves >50% WNS improvement
[ ] ASAP7 demo achieves >40% WNS improvement
[ ] Sky130 demo achieves >50% WNS improvement
[ ] At least 1 doomed case terminated early in each demo
[ ] All artifacts present and valid
[ ] summary.json shows {"mocking": false, "placeholders": false}
[ ] verify_demo_output.sh passes without errors

╔══════════════════════════════════════════════════════════════════════════════╗
║                   ALL BOXES MUST BE CHECKED FOR PRODUCTION                   ║
╚══════════════════════════════════════════════════════════════════════════════╝
```

---

## CRITICAL MISSING FEATURES (MUST BE IMPLEMENTED)

**⚠️ WARNING**: The following features are ABSOLUTELY REQUIRED for the demo to work.
Previous implementations created infrastructure that runs but produces NO IMPROVEMENT
because these core features were missing.

### Problem Statement

The current implementation has these critical gaps:

1. **ECO Selection uses placeholders** - Code uses `trial_eco_0`, `trial_eco_1` instead of real ECOs
2. **Trial scripts don't apply ECOs** - Only run STA, never call repair_design/repair_timing
3. **All trials identical** - Same script, same result, 0% improvement
4. **Snapshots read-only** - Modified ODB not saved or passed to next stage
5. **Extreme snapshot not extreme** - WNS is -316ps instead of expected -2000ps
6. **No doom detection** - No cases terminated early
7. **No prior learning** - ~/.noodle2/priors/ doesn't exist
8. **Placeholder visualizations** - pareto_frontier.png, eco_effectiveness_chart.png are 0 bytes

### 1. ECO Selection and Injection (CRITICAL)

**File**: `src/controller/eco_selector.py`

The ECO selector MUST:
- Select REAL ECOs (BufferInsertionECO, CellResizeECO, etc.) not placeholders
- Generate VARIED parameters for each trial
- Consider auto-diagnosis results when prioritizing ECOs
- Deprioritize suspicious ECOs based on priors

```python
class ECOSelector:
    def select_eco(self, stage_config: StageConfig, trial_index: int, diagnosis: DiagnosisResult) -> ECOConfig:
        """
        Select a REAL ECO with VARIED parameters.

        MUST NOT return placeholder names like 'trial_eco_0'.
        MUST return actual ECO classes with varied parameters.
        """
        # Filter by allowed ECO classes for this stage
        allowed_ecos = [BufferInsertionECO, CellResizeECO, GateCloningECO, PlacementRefinementECO]
        eligible = [e for e in allowed_ecos if e.eco_class in stage_config.eco_classes]

        # Prioritize based on diagnosis
        if diagnosis.primary_issue == "timing":
            if diagnosis.timing.is_wire_dominated:
                eligible.sort(key=lambda e: 0 if e == BufferInsertionECO else 1)
            else:
                eligible.sort(key=lambda e: 0 if e == CellResizeECO else 1)

        # Select ECO and generate varied parameters
        eco_class = eligible[trial_index % len(eligible)]
        params = self._generate_varied_params(eco_class, trial_index)

        return ECOConfig(eco_class=eco_class, parameters=params)

    def _generate_varied_params(self, eco_class, trial_index: int) -> dict:
        """Generate DIFFERENT parameters for each trial."""
        if eco_class == BufferInsertionECO:
            margins = [10, 20, 50, 100, 200]  # ps
            max_pcts = [5, 10, 15, 20, 25]
            return {
                "slack_margin_ps": margins[trial_index % len(margins)],
                "max_buffer_percent": max_pcts[trial_index % len(max_pcts)],
            }
        elif eco_class == CellResizeECO:
            wire_lens = [50, 75, 100, 150, 200]
            utils = [0.7, 0.75, 0.8, 0.85, 0.9]
            return {
                "max_wire_length": wire_lens[trial_index % len(wire_lens)],
                "max_utilization": utils[trial_index % len(utils)],
            }
        # ... etc for other ECOs
```

### 2. ECO TCL Injection into Trial Scripts (CRITICAL)

**File**: `src/trial_runner/tcl_generator.py`

Each trial script MUST:
1. Load the design (read_db)
2. Capture PRE-ECO metrics
3. **APPLY THE ECO** (repair_design, repair_timing, etc.)
4. Capture POST-ECO metrics
5. **SAVE THE MODIFIED DESIGN** (write_db)

```tcl
# CORRECT trial script structure:

# 1. Load design
read_db $input_odb

# 2. Pre-ECO metrics
set pre_wns [sta::worst_slack -max]

# 3. APPLY ECO (THE CRITICAL PART)
repair_timing -setup -slack_margin 0.05 -max_buffer_percent 20

# 4. Post-ECO metrics
estimate_parasitics -placement
set post_wns [sta::worst_slack -max]

# 5. SAVE MODIFIED DESIGN
write_db $output_odb
```

**WRONG** (current implementation):
```tcl
# Only runs STA, no ECO applied, no design saved
read_db $odb_file
report_checks
# MISSING: repair_timing
# MISSING: write_db
```

### 3. Modified Snapshot Propagation (CRITICAL)

**File**: `src/controller/executor.py`

After a trial succeeds, its modified ODB MUST become the input for the next stage:

```python
def _select_survivors(self, stage_result: StageResult) -> list[Case]:
    survivors = []
    for trial in sorted_trials[:num_survivors]:
        # CRITICAL: Use the MODIFIED ODB path, not the original
        survivor = Case(
            case_id=trial.trial_id,
            snapshot_path=trial.modified_odb_path,  # NOT original snapshot
            metrics=trial.metrics,
        )
        survivors.append(survivor)
    return survivors
```

### 4. Extreme Snapshot Generation (CRITICAL)

**Problem**: Current "extreme" snapshots are just copies of base snapshots with aggressive
clock constraints. The ODB files are IDENTICAL (same MD5 hash). This doesn't create true
extreme cases because the design was placed with normal timing - it's well optimized.

**Root Cause**:
```
nangate45_base/gcd_placed.odb:    ab6beae63343354a7b7095a67614c5b7
nangate45_extreme/gcd_placed.odb: ab6beae63343354a7b7095a67614c5b7  <-- IDENTICAL!
```

**Required Solution**: Generate NEW extreme snapshots with:
1. **High utilization** (85%+) - causes wire congestion
2. **Aggressive clock during placement** - optimizer can't meet timing
3. **Larger design** - GCD is too small for dramatic violations

**File**: `scripts/generate_extreme_snapshots.tcl`

```tcl
# Generate TRULY extreme Nangate45 snapshot
# This must be run ONCE to create the extreme ODB

# Load PDK
read_lef /OpenROAD-flow-scripts/flow/platforms/nangate45/lef/NangateOpenCellLibrary.tech.lef
read_lef /OpenROAD-flow-scripts/flow/platforms/nangate45/lef/NangateOpenCellLibrary.macro.mod.lef
read_liberty /OpenROAD-flow-scripts/flow/platforms/nangate45/lib/NangateOpenCellLibrary_typical.lib

# Use a LARGER design (ibex or aes) for more dramatic violations
# GCD is too small - only ~100 cells
read_verilog /OpenROAD-flow-scripts/flow/designs/nangate45/ibex/ibex.v
link_design ibex_core

# Set EXTREMELY aggressive clock BEFORE placement
# This forces the placer to fail at meeting timing
create_clock -name clk -period 0.3 [get_ports clk_i]

# Initialize with HIGH utilization (85%) - causes congestion
initialize_floorplan \
    -utilization 0.85 \
    -aspect_ratio 1.0 \
    -core_space 2.0

# Run placement with aggressive timing - will have violations
global_placement -timing_driven -density 0.85
detailed_placement

# Verify we have REAL extreme violations
set wns [sta::worst_slack -max]
set wns_ps [expr {int($wns * 1000)}]

puts "Generated extreme snapshot:"
puts "  WNS: $wns_ps ps"

if {$wns_ps > -1500} {
    puts "WARNING: WNS not extreme enough!"
    puts "Expected: < -1500 ps"
    puts "Got: $wns_ps ps"
    puts "Try: smaller clock period or higher utilization"
}

# Save the TRULY extreme snapshot
write_db /work/ibex_extreme.odb

puts "Extreme snapshot saved to: ibex_extreme.odb"
puts "This ODB should be DIFFERENT from base case ODB"
```

**Alternative for GCD (if must use GCD)**:

```tcl
# For GCD, use 50ps clock (20GHz) - physically impossible but creates violations
create_clock -name clk -period 0.05 [get_ports clk]

# AND use 90% utilization
initialize_floorplan -utilization 0.90

# This should create WNS around -1500 to -2000ps even for small GCD
```

**Verification**:
```bash
# Extreme ODB MUST be different from base
md5sum studies/nangate45_base/gcd_placed.odb
md5sum studies/nangate45_extreme/gcd_extreme.odb
# These MUST be different hashes!

# Verify extreme WNS
docker run --rm -v $(pwd)/studies/nangate45_extreme:/snapshot openroad/orfs:latest \
    openroad -exit -cmd "read_db /snapshot/gcd_extreme.odb; \
    create_clock -period 0.5 [get_ports clk]; \
    puts [sta::worst_slack -max]"
# Expected: < -1.5 (i.e., < -1500ps)
```

### 5. Doom Detection Implementation (CRITICAL)

**File**: `src/controller/doom_detector.py`

Doom detection MUST actually terminate cases:

```python
class DoomDetector:
    def check_doom(self, case_id: str, metrics: Metrics) -> Optional[DoomDecision]:
        # Metric-hopeless
        if metrics.wns_ps < -10000:
            return DoomDecision(
                doom_type="metric_hopeless",
                reason=f"WNS {metrics.wns_ps}ps is unrecoverable",
                action="terminate_immediately",
            )

        # Trajectory doom (3 stages with <2% improvement)
        if self._check_stagnation(case_id, metrics):
            return DoomDecision(
                doom_type="trajectory_doom",
                reason="3 consecutive stages with <2% improvement",
                action="terminate",
            )

        return None

# In executor.py, ACTUALLY USE IT:
doom = self.doom_detector.check_doom(case_id, metrics)
if doom:
    self.terminated_cases.add(case_id)
    print(f"[DOOM] {case_id} terminated: {doom.reason}")
```

### 6. Prior Learning Persistence (CRITICAL)

**File**: `src/controller/prior_repository.py`

Priors MUST be persisted to disk and updated after each trial:

```python
class PriorRepository:
    def __init__(self):
        self.db_path = Path.home() / ".noodle2" / "priors" / "priors.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

    def update_prior(self, eco_name: str, success: bool, improvement: float):
        """MUST be called after EVERY trial."""
        # Update SQLite database
        # Change state: unknown -> trusted/suspicious based on success rate
```

### 7. Real Visualization Generation (CRITICAL)

**Files**: `src/visualization/pareto.py`, `src/visualization/eco_charts.py`

All visualization files MUST be > 10KB (not 0 bytes):

```python
def generate_pareto_frontier(trials: list, output_png: Path) -> bool:
    # MUST generate a real PNG with matplotlib
    # MUST verify file size > 1000 bytes
    plt.savefig(output_png, dpi=150)
    if output_png.stat().st_size < 1000:
        raise ValueError(f"Generated PNG too small: {output_png}")
    return True
```

---

## Demo Success Criteria (MANDATORY)

For the demo to convince a senior director:

### Quantitative

| Metric | Nangate45 | ASAP7 | Sky130 |
|--------|-----------|-------|--------|
| Initial WNS | < -1500ps | < -1000ps | < -2000ps |
| WNS Improvement | > 50% | > 40% | > 50% |
| Trials with unique metrics | 100% | 100% | 100% |
| Doomed cases terminated | ≥ 1 | ≥ 1 | ≥ 1 |

### Console Output Must Show

```
[ECO] Applied BufferInsertionECO to case_0_3
      WNS Before: -1850 ps → After: -920 ps (50.3% improvement)

[DOOM] Case extreme_0_5 terminated: trajectory_doom
       Compute saved: ~12 trials

[PRIOR] CellResizeECO: 15S/3F -> state=trusted
```

### Verification Commands

```bash
# 1. Verify trials produce different results
jq '.wns_ps' artifacts/*/trial_*/metrics.json | sort -u | wc -l
# Expected: > 1 (not all identical)

# 2. Verify ECOs actually applied
grep "repair_timing\|repair_design" artifacts/*/trial_*/trial_script.tcl
# Expected: Commands present in scripts

# 3. Verify visualizations are real
find demo_output -name "*.png" -size 0
# Expected: No output (no empty files)

# 4. Verify doom detection
grep -r "DOOM\|terminated" demo_output/*/summary.json
# Expected: At least one doom event

# 5. Verify priors exist
test -f ~/.noodle2/priors/priors.db && echo "OK" || echo "MISSING"
# Expected: OK
```

---

## Handover Checklist

To recreate this system from this app_spec.txt:

1. ☐ Read entire spec (all 4800+ lines)
2. ☐ Implement core infrastructure (Study, Case, Stage, Trial)
3. ☐ Implement Docker trial execution with ORFS
4. ☐ Implement ECO classes with generate_tcl() methods
5. ☐ **Implement ECO selection with REAL ECOs and VARIED parameters**
6. ☐ **Implement ECO TCL injection into trial scripts**
7. ☐ **Implement modified snapshot propagation between stages**
8. ☐ **Generate extreme snapshots with WNS < -1500ps**
9. ☐ Implement doom detection that ACTUALLY terminates cases
10. ☐ Implement prior learning with SQLite persistence
11. ☐ Implement visualization with REAL data (not 0-byte files)
12. ☐ Run demos and verify success criteria
13. ☐ Run all verification commands

**The single most important thing**: Every trial MUST apply a REAL ECO that
ACTUALLY MODIFIES the design and produces DIFFERENT metrics.

---

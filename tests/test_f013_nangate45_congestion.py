"""Tests for Feature F013: Parse congestion metrics from Nangate45 base case.

Feature F013: Congestion metrics can be parsed from global_route output for Nangate45
Depends on: F009 (Nangate45 base case runs successfully)

Test Steps:
1. Run Nangate45 base case with global_route
2. Parse congestion_report.txt
3. Extract bins_total (total routing bins)
4. Extract bins_hot (bins with overflow)
5. Calculate hot_ratio = bins_hot / bins_total
6. Verify hot_ratio is between 0.0 and 1.0
"""

import json
import tempfile
from pathlib import Path

import pytest

from src.controller.types import ExecutionMode
from src.parsers.congestion import (
    parse_congestion_report,
    parse_congestion_report_file,
    parse_congestion_json,
    format_congestion_summary,
)
from src.trial_runner.tcl_generator import generate_trial_script
from src.trial_runner.trial import Trial, TrialConfig


class TestF013NangateCongestionParsing:
    """Tests for F013: Parse congestion metrics from Nangate45."""

    def test_step_1_generate_sta_congestion_script_for_nangate45(
        self, tmp_path: Path
    ) -> None:
        """
        Step 1: Generate STA+Congestion script for Nangate45.

        This script should include global_route command and congestion report generation.
        """
        script_content = generate_trial_script(
            execution_mode=ExecutionMode.STA_CONGESTION,
            design_name="counter",
            output_dir=str(tmp_path),
            clock_period_ns=10.0,
            pdk="nangate45",
        )

        # Verify script contains global routing command
        assert "global_route" in script_content
        assert "congestion_report" in script_content

        # Verify it mentions congestion report file
        assert "congestion_report.txt" in script_content

        # Verify it's set up for Nangate45
        assert "nangate45" in script_content.lower()

        print("✓ Generated STA+Congestion script for Nangate45")
        print(f"  Script contains global_route command: Yes")
        print(f"  Script generates congestion_report.txt: Yes")

    def test_step_2_parse_congestion_report_from_content(self) -> None:
        """
        Step 2: Parse congestion_report.txt content.

        Verify parser can extract bins_total, bins_hot, and other metrics.
        """
        # Sample congestion report in OpenROAD format (as generated by TCL script)
        report_content = """Global routing congestion report
Design: counter

Total bins: 1024
Overflow bins: 45
Max overflow: 12

Per-layer congestion:
  Layer metal2 overflow: 15
  Layer metal3 overflow: 18
  Layer metal4 overflow: 12

Routing utilization: 85.2%
Congestion hotspots: 45 bins exceed 90% capacity
"""

        metrics = parse_congestion_report(report_content)

        # Verify bins_total is extracted
        assert metrics.bins_total == 1024
        print(f"✓ Extracted bins_total: {metrics.bins_total}")

        # Verify bins_hot is extracted
        assert metrics.bins_hot == 45
        print(f"✓ Extracted bins_hot: {metrics.bins_hot}")

        # Verify hot_ratio is calculated
        assert metrics.hot_ratio == 45 / 1024
        print(f"✓ Calculated hot_ratio: {metrics.hot_ratio:.4f}")

        # Verify max_overflow is extracted
        assert metrics.max_overflow == 12
        print(f"✓ Extracted max_overflow: {metrics.max_overflow}")

        # Verify layer metrics are extracted
        assert "metal2" in metrics.layer_metrics
        assert "metal3" in metrics.layer_metrics
        assert "metal4" in metrics.layer_metrics
        assert metrics.layer_metrics["metal2"] == 15
        assert metrics.layer_metrics["metal3"] == 18
        assert metrics.layer_metrics["metal4"] == 12
        print(f"✓ Extracted layer metrics: {len(metrics.layer_metrics)} layers")

    def test_step_3_extract_bins_total(self) -> None:
        """
        Step 3: Verify bins_total extraction works correctly.
        """
        # Test various formats
        test_cases = [
            ("Total bins: 1024", 1024),
            ("total bins = 1024", 1024),
            ("TOTAL BINS: 2048", 2048),
            ("Total bins   :   4096", 4096),
        ]

        for report, expected_bins in test_cases:
            metrics = parse_congestion_report(report)
            assert metrics.bins_total == expected_bins
            print(f"✓ Parsed '{report.strip()}' → bins_total={expected_bins}")

    def test_step_4_extract_bins_hot(self) -> None:
        """
        Step 4: Verify bins_hot (overflow bins) extraction works correctly.
        """
        # Test various formats
        test_cases = [
            ("Total bins: 1024\nOverflow bins: 45", 45),
            ("Total bins: 1024\nHot bins: 30", 30),
            ("Total bins: 1024\nBins with overflow: 100", 100),
            ("Total bins: 1024\noverflow bins = 75", 75),
        ]

        for report, expected_hot in test_cases:
            metrics = parse_congestion_report(report)
            assert metrics.bins_hot == expected_hot
            print(f"✓ Extracted bins_hot={expected_hot} from report")

    def test_step_5_calculate_hot_ratio(self) -> None:
        """
        Step 5: Verify hot_ratio calculation.

        hot_ratio = bins_hot / bins_total
        """
        test_cases = [
            (1024, 45, 45 / 1024),
            (2048, 100, 100 / 2048),
            (4096, 0, 0.0),
            (1000, 500, 0.5),
            (1000, 1000, 1.0),
        ]

        for bins_total, bins_hot, expected_ratio in test_cases:
            report = f"Total bins: {bins_total}\nOverflow bins: {bins_hot}"
            metrics = parse_congestion_report(report)

            assert abs(metrics.hot_ratio - expected_ratio) < 1e-9
            print(f"✓ hot_ratio = {bins_hot}/{bins_total} = {metrics.hot_ratio:.4f}")

    def test_step_6_verify_hot_ratio_bounds(self) -> None:
        """
        Step 6: Verify hot_ratio is always between 0.0 and 1.0.
        """
        test_cases = [
            (1024, 0),      # No congestion
            (1024, 45),     # Light congestion
            (1024, 512),    # Moderate congestion
            (1024, 1024),   # Full congestion
        ]

        for bins_total, bins_hot in test_cases:
            report = f"Total bins: {bins_total}\nOverflow bins: {bins_hot}"
            metrics = parse_congestion_report(report)

            # Verify hot_ratio is in valid range
            assert 0.0 <= metrics.hot_ratio <= 1.0, (
                f"hot_ratio {metrics.hot_ratio} out of bounds for "
                f"bins_hot={bins_hot}, bins_total={bins_total}"
            )

            # Verify calculation is exact
            expected_ratio = bins_hot / bins_total
            assert abs(metrics.hot_ratio - expected_ratio) < 1e-9

            print(f"✓ Valid hot_ratio: {metrics.hot_ratio:.4f} "
                  f"(bins_hot={bins_hot}, bins_total={bins_total})")

    def test_parse_from_file(self, tmp_path: Path) -> None:
        """Test parsing congestion report from file."""
        report_file = tmp_path / "congestion_report.txt"
        report_file.write_text("""
Total bins: 1024
Overflow bins: 45
Max overflow: 12
""")

        metrics = parse_congestion_report_file(str(report_file))

        assert metrics.bins_total == 1024
        assert metrics.bins_hot == 45
        assert metrics.hot_ratio == 45 / 1024
        assert metrics.max_overflow == 12

        print(f"✓ Parsed congestion report from file: {report_file}")

    def test_parse_from_json_format(self) -> None:
        """Test parsing congestion metrics from JSON format."""
        json_data = {
            "bins_total": 1024,
            "bins_hot": 45,
            "max_overflow": 12,
            "layer_metrics": {
                "metal2": 15,
                "metal3": 18,
                "metal4": 12,
            }
        }

        metrics = parse_congestion_json(json_data)

        assert metrics.bins_total == 1024
        assert metrics.bins_hot == 45
        assert metrics.hot_ratio == 45 / 1024
        assert metrics.max_overflow == 12
        assert len(metrics.layer_metrics) == 3

        print("✓ Parsed congestion metrics from JSON format")

    def test_format_congestion_summary(self) -> None:
        """Test formatting congestion metrics as human-readable summary."""
        report = """Total bins: 1024
Overflow bins: 45
Max overflow: 12
Layer metal2 overflow: 15
Layer metal3 overflow: 18
"""

        metrics = parse_congestion_report(report)
        summary = format_congestion_summary(metrics)

        # Verify summary contains key information
        assert "Total bins: 1024" in summary
        assert "Hot bins: 45" in summary
        assert "Hot ratio:" in summary
        assert "Max overflow: 12" in summary
        assert "metal2: 15" in summary
        assert "metal3: 18" in summary

        print("✓ Formatted congestion summary:")
        print(summary)

    def test_edge_case_zero_congestion(self) -> None:
        """Test parsing report with zero congestion (no overflow bins)."""
        report = """Total bins: 1024
Overflow bins: 0
"""

        metrics = parse_congestion_report(report)

        assert metrics.bins_total == 1024
        assert metrics.bins_hot == 0
        assert metrics.hot_ratio == 0.0

        print("✓ Handled zero congestion case correctly")

    def test_edge_case_missing_bins_hot_defaults_to_zero(self) -> None:
        """Test that missing bins_hot field defaults to 0 (no congestion)."""
        report = """Total bins: 1024
Max overflow: 0
"""

        metrics = parse_congestion_report(report)

        # Should default bins_hot to 0 if not found
        assert metrics.bins_hot == 0
        assert metrics.hot_ratio == 0.0

        print("✓ Missing bins_hot defaults to 0 (no congestion)")

    def test_error_missing_bins_total(self) -> None:
        """Test that missing bins_total raises error."""
        report = """Overflow bins: 45
Max overflow: 12
"""

        with pytest.raises(ValueError, match="Could not parse total bins"):
            parse_congestion_report(report)

        print("✓ Missing bins_total raises ValueError as expected")

    def test_error_file_not_found(self, tmp_path: Path) -> None:
        """Test that missing file raises FileNotFoundError."""
        nonexistent_file = tmp_path / "nonexistent.txt"

        with pytest.raises(FileNotFoundError):
            parse_congestion_report_file(str(nonexistent_file))

        print("✓ Missing file raises FileNotFoundError as expected")


class TestF013Integration:
    """Integration tests for F013 with Nangate45 workflow."""

    def test_complete_nangate45_congestion_workflow(self, tmp_path: Path) -> None:
        """
        Complete workflow test: Generate script → Execute → Parse metrics.

        This test simulates the complete F013 workflow:
        1. Generate STA+Congestion script for Nangate45
        2. Script generates congestion_report.txt
        3. Parse the report
        4. Extract bins_total, bins_hot, hot_ratio
        5. Verify all metrics are valid
        """
        # Step 1: Generate script
        script_content = generate_trial_script(
            execution_mode=ExecutionMode.STA_CONGESTION,
            design_name="counter",
            output_dir=str(tmp_path),
            clock_period_ns=10.0,
            pdk="nangate45",
        )

        assert "global_route" in script_content
        assert "congestion_report.txt" in script_content
        print("✓ Step 1: Generated STA+Congestion script")

        # Step 2: Simulate congestion report (would be generated by OpenROAD)
        report_file = tmp_path / "congestion_report.txt"
        report_file.write_text("""Global routing congestion report
Design: counter

Total bins: 1024
Overflow bins: 45
Max overflow: 12

Per-layer congestion:
  Layer metal2 overflow: 15
  Layer metal3 overflow: 18
  Layer metal4 overflow: 12
""")
        print("✓ Step 2: Congestion report generated")

        # Step 3: Parse the report
        metrics = parse_congestion_report_file(str(report_file))
        print("✓ Step 3: Parsed congestion report")

        # Step 4: Verify bins_total
        assert metrics.bins_total == 1024
        print(f"✓ Step 4a: bins_total = {metrics.bins_total}")

        # Verify bins_hot
        assert metrics.bins_hot == 45
        print(f"✓ Step 4b: bins_hot = {metrics.bins_hot}")

        # Step 5: Verify hot_ratio calculation
        expected_ratio = 45 / 1024
        assert abs(metrics.hot_ratio - expected_ratio) < 1e-9
        print(f"✓ Step 5: hot_ratio = {metrics.hot_ratio:.6f}")

        # Step 6: Verify hot_ratio is in valid range
        assert 0.0 <= metrics.hot_ratio <= 1.0
        print(f"✓ Step 6: hot_ratio is in valid range [0.0, 1.0]")

        # Verify layer metrics
        assert len(metrics.layer_metrics) == 3
        assert metrics.layer_metrics["metal2"] == 15
        assert metrics.layer_metrics["metal3"] == 18
        assert metrics.layer_metrics["metal4"] == 12
        print(f"✓ Bonus: Parsed {len(metrics.layer_metrics)} layer metrics")

        print("\n=== F013 Complete Workflow Test PASSED ===")
        print(f"Nangate45 congestion metrics successfully parsed:")
        print(f"  - bins_total: {metrics.bins_total}")
        print(f"  - bins_hot: {metrics.bins_hot}")
        print(f"  - hot_ratio: {metrics.hot_ratio:.4f} ({metrics.hot_ratio*100:.2f}%)")
        print(f"  - max_overflow: {metrics.max_overflow}")
        print(f"  - Per-layer metrics: {', '.join(metrics.layer_metrics.keys())}")

    def test_metrics_json_includes_congestion_fields(self, tmp_path: Path) -> None:
        """
        Verify that metrics.json includes congestion fields.

        The TCL script should write bins_total, bins_hot, and hot_ratio to metrics.json.
        """
        # Generate script
        script_content = generate_trial_script(
            execution_mode=ExecutionMode.STA_CONGESTION,
            design_name="counter",
            output_dir=str(tmp_path),
            clock_period_ns=10.0,
            pdk="nangate45",
        )

        # Verify script writes these fields to metrics.json
        assert "bins_total" in script_content
        assert "bins_hot" in script_content
        assert "hot_ratio" in script_content

        print("✓ Script writes bins_total, bins_hot, hot_ratio to metrics.json")

        # Simulate metrics.json output
        metrics_file = tmp_path / "metrics.json"
        metrics_data = {
            "design": "counter",
            "execution_mode": "sta_congestion",
            "wns_ps": 2800,
            "tns_ps": 0,
            "bins_total": 1024,
            "bins_hot": 45,
            "hot_ratio": 45.0 / 1024.0,
            "max_overflow": 12,
            "status": "success"
        }
        metrics_file.write_text(json.dumps(metrics_data, indent=2))

        # Parse and verify
        loaded_metrics = json.loads(metrics_file.read_text())
        assert loaded_metrics["bins_total"] == 1024
        assert loaded_metrics["bins_hot"] == 45
        assert abs(loaded_metrics["hot_ratio"] - (45.0 / 1024.0)) < 1e-9

        print("✓ metrics.json includes all congestion fields")
        print(f"  - bins_total: {loaded_metrics['bins_total']}")
        print(f"  - bins_hot: {loaded_metrics['bins_hot']}")
        print(f"  - hot_ratio: {loaded_metrics['hot_ratio']:.6f}")
